{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3q6mEQ35-9Dg"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Phase1_Ingestion_Cleaning\")\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "INPUT_CSV = \"/path/to/traffic_data_large.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def find_file(filename=\"traffic_data_large.csv\", search_roots=(\"/\", )):\n",
        "    matches = []\n",
        "    for root in search_roots:\n",
        "        root_path = Path(root)\n",
        "        if root_path.exists():\n",
        "            for p in root_path.rglob(filename):\n",
        "                # Skip system, proc, dev, etc. for speed\n",
        "                if any(str(p).startswith(s) for s in [\"/proc\", \"/sys\", \"/dev\", \"/run\", \"/snap\"]):\n",
        "                    continue\n",
        "                matches.append(str(p))\n",
        "                if len(matches) >= 5:\n",
        "                    break\n",
        "    return matches\n",
        "\n",
        "candidates = find_file(\"traffic_data_large.csv\", search_roots=(\".\", \"/content\", \"/home\", \"/Workspace\", \"/dbfs\", \"/FileStore\"))\n",
        "print(\"Candidate paths:\", candidates)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRP1fxaEBTxJ",
        "outputId": "fa78d1de-3147-4400-8719-d387b9fa7519"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate paths: ['traffic_data_large.csv', '/content/traffic_data_large.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INPUT_CSV = \"/content/traffic_data_large.csv\"\n"
      ],
      "metadata": {
        "id": "jMoouUtpB1Fj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, pathlib\n",
        "print(\"INPUT_CSV =\", INPUT_CSV)\n",
        "print(\"Absolute:\", os.path.isabs(INPUT_CSV))\n",
        "print(\"Exists (Python):\", os.path.exists(INPUT_CSV))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hEc00ebA9DM",
        "outputId": "e6bdd05e-5fef-437c-893c-318e6ea6a322"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT_CSV = /content/traffic_data_large.csv\n",
            "Absolute: True\n",
            "Exists (Python): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "try:\n",
        "    _test_df = (spark.read\n",
        "        .option(\"header\", True)\n",
        "        .option(\"inferSchema\", \"false\")\n",
        "        .csv(INPUT_CSV)\n",
        "    )\n",
        "    print(\"Spark can read INPUT_CSV directly \")\n",
        "except AnalysisException as e:\n",
        "    print(\"Direct read failed. Trying file:// URI …\")\n",
        "\n",
        "    if not INPUT_CSV.startswith(\"dbfs:\") and not INPUT_CSV.startswith(\"hdfs:\") and not INPUT_CSV.startswith(\"s3:\"):\n",
        "        uri = \"file://\" + INPUT_CSV if INPUT_CSV.startswith(\"/\") else \"file://\" + str(pathlib.Path.cwd() / INPUT_CSV)\n",
        "        print(\"Trying URI:\", uri)\n",
        "        _test_df = (spark.read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"inferSchema\", \"false\")\n",
        "            .csv(uri)\n",
        "        )\n",
        "        INPUT_CSV = uri\n",
        "        print(\"Spark can read via file:// \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqaCLlFcCEoO",
        "outputId": "0988b959-ee9e-4a83-b6d4-8212e405f5de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark can read INPUT_CSV directly ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Phase 1:\n",
        "df_raw = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", \"false\")\n",
        "    .csv(INPUT_CSV)\n",
        ")\n",
        "\n",
        "\n",
        "for c in df_raw.columns:\n",
        "    df_raw = df_raw.withColumn(c, F.trim(F.col(c)))\n",
        "\n",
        "print(\"Schema (raw):\")\n",
        "df_raw.printSchema()\n",
        "print(\"Row count (raw):\", df_raw.count())\n",
        "df_raw.show(10, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYS-5Y-eCMhX",
        "outputId": "c6f0ec6e-9812-4b04-9d1f-0649da3dc3f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema (raw):\n",
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count: string (nullable = true)\n",
            " |-- avg_speed: string (nullable = true)\n",
            " |-- temperature: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "Row count (raw): 500000\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|sensor_id|location |road_name      |vehicle_count|avg_speed|temperature|timestamp          |status  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|S105     |Chennai  |OMR            |invalid      |NULL     |39         |12/01/2026 06:00:00|INACTIVE|\n",
            "|S113     |Chennai  |Mount Road     |103          |73.5     |36         |2026-01-12 06:00:05|ACTIVE  |\n",
            "|S228     |Delhi    |Janpath        |16           |20.0     |35         |2026-01-12 06:00:10|ACTIVE  |\n",
            "|S160     |Bangalore|MG Road        |27           |27.1     |32         |2026-01-12 06:00:15|ACTIVE  |\n",
            "|S252     |Mumbai   |Western Express|115          |59.3     |39         |2026-01-12 06:00:20|ACTIVE  |\n",
            "|S134     |Kolkata  |EM Bypass      |13           |23.6     |29         |2026-01-12 06:00:25|ACTIVE  |\n",
            "|S246     |Delhi    |Janpath        |81           |71.1     |33         |2026-01-12 06:00:30|ACTIVE  |\n",
            "|S154     |Delhi    |Janpath        |40           |36.3     |32         |2026-01-12 06:00:35|ACTIVE  |\n",
            "|S227     |Delhi    |Ring Road      |37           |49.3     |39         |2026-01-12 06:00:40|ACTIVE  |\n",
            "|S221     |Chennai  |GST Road       |89           |79.3     |36         |2026-01-12 06:00:45|ACTIVE  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Preserve original timestamp for audit\n",
        "df = df_raw.withColumn(\"timestamp_original\", F.col(\"timestamp\"))\n",
        "\n",
        "# vehicle_count → vehicle_count_clean\n",
        "df = (df\n",
        "    .withColumn(\"vc_lower\", F.lower(F.col(\"vehicle_count\")))\n",
        "    .withColumn(\n",
        "        \"vehicle_count_clean\",\n",
        "        F.when(\n",
        "            F.col(\"vc_lower\").isNull() | (F.col(\"vc_lower\") == \"\") | (F.col(\"vc_lower\") == \"invalid\"),\n",
        "            F.lit(None).cast(\"int\")\n",
        "        ).when(\n",
        "            F.col(\"vc_lower\").rlike(r\"^\\\\d+$\"),\n",
        "            F.col(\"vc_lower\").cast(IntegerType())\n",
        "        ).otherwise(F.lit(None).cast(\"int\"))\n",
        "    )\n",
        "    .drop(\"vc_lower\")\n",
        ")\n",
        "\n",
        "# avg_speed → avg_speed_clean\n",
        "df = (df\n",
        "    .withColumn(\"as_lower\", F.lower(F.col(\"avg_speed\")))\n",
        "    .withColumn(\n",
        "        \"avg_speed_clean\",\n",
        "        F.when(\n",
        "            F.col(\"as_lower\").isNull() | (F.col(\"as_lower\") == \"\"),\n",
        "            F.lit(None).cast(\"double\")\n",
        "        ).otherwise(F.col(\"as_lower\").cast(\"double\"))\n",
        "    )\n",
        "    .drop(\"as_lower\")\n",
        ")\n",
        "\n",
        "# timestamp → event_time (three formats)\n",
        "ts1 = F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "ts2 = F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\")\n",
        "ts3 = F.to_timestamp(\"timestamp\", \"yyyy/MM/dd HH:mm:ss\")\n",
        "df = df.withColumn(\"event_time\", F.coalesce(ts1, ts2, ts3))\n",
        "\n",
        "df.select(\"sensor_id\",\"location\",\"road_name\",\"vehicle_count\",\"vehicle_count_clean\",\n",
        "          \"avg_speed\",\"avg_speed_clean\",\"timestamp\",\"event_time\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "RAIiwJukCli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2"
      ],
      "metadata": {
        "id": "Y6TIyz-GEOkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 Trim all string columns\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_trimmed = df_raw\n",
        "for c in df_trimmed.columns:\n",
        "    df_trimmed = df_trimmed.withColumn(c, F.trim(F.col(c)))\n",
        "\n",
        "df_trimmed.printSchema()\n",
        "df_trimmed.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOJNR9m5ESDV",
        "outputId": "95dd24e6-2738-4f44-97b1-4d8357083e8c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count: string (nullable = true)\n",
            " |-- avg_speed: string (nullable = true)\n",
            " |-- temperature: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|sensor_id|location |road_name      |vehicle_count|avg_speed|temperature|timestamp          |status  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|S105     |Chennai  |OMR            |invalid      |NULL     |39         |12/01/2026 06:00:00|INACTIVE|\n",
            "|S113     |Chennai  |Mount Road     |103          |73.5     |36         |2026-01-12 06:00:05|ACTIVE  |\n",
            "|S228     |Delhi    |Janpath        |16           |20.0     |35         |2026-01-12 06:00:10|ACTIVE  |\n",
            "|S160     |Bangalore|MG Road        |27           |27.1     |32         |2026-01-12 06:00:15|ACTIVE  |\n",
            "|S252     |Mumbai   |Western Express|115          |59.3     |39         |2026-01-12 06:00:20|ACTIVE  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2 Clean vehicle_count\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_vc = (\n",
        "    df_trimmed\n",
        "\n",
        "    .withColumn(\"vehicle_count_lower\", F.lower(F.col(\"vehicle_count\")))\n",
        "\n",
        "    .withColumn(\n",
        "        \"vehicle_count_clean\",\n",
        "        F.when(\n",
        "            F.col(\"vehicle_count_lower\").isNull() |\n",
        "            (F.col(\"vehicle_count_lower\") == \"\") |\n",
        "            (F.col(\"vehicle_count_lower\") == \"invalid\"),\n",
        "            F.lit(None).cast(IntegerType())\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"vehicle_count_lower\").rlike(r\"^\\d+$\"),\n",
        "            F.col(\"vehicle_count_lower\").cast(IntegerType())\n",
        "        )\n",
        "        .otherwise(F.lit(None).cast(IntegerType()))\n",
        "    )\n",
        "    .drop(\"vehicle_count_lower\")\n",
        ")\n",
        "\n",
        "df_vc.select(\"vehicle_count\", \"vehicle_count_clean\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYwEljguEa0U",
        "outputId": "eb1eac61-bd07-4985-9295-a2bf416c37ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------------+\n",
            "|vehicle_count|vehicle_count_clean|\n",
            "+-------------+-------------------+\n",
            "|invalid      |NULL               |\n",
            "|103          |103                |\n",
            "|16           |16                 |\n",
            "|27           |27                 |\n",
            "|115          |115                |\n",
            "|13           |13                 |\n",
            "|81           |81                 |\n",
            "|40           |40                 |\n",
            "|37           |37                 |\n",
            "|89           |89                 |\n",
            "+-------------+-------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3 Clean avg_speed: empty -> null ; cast to DoubleType\n",
        "df_speed = (\n",
        "    df_vc\n",
        "    .withColumn(\"avg_speed_lower\", F.lower(F.col(\"avg_speed\")))\n",
        "    .withColumn(\n",
        "        \"avg_speed_clean\",\n",
        "        F.when(\n",
        "            F.col(\"avg_speed_lower\").isNull() |\n",
        "            (F.col(\"avg_speed_lower\") == \"\"),\n",
        "            F.lit(None).cast(\"double\")\n",
        "        )\n",
        "        .otherwise(F.col(\"avg_speed_lower\").cast(\"double\"))\n",
        "    )\n",
        "    .drop(\"avg_speed_lower\")\n",
        ")\n",
        "\n",
        "df_speed.select(\"avg_speed\", \"avg_speed_clean\").show(10, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs5CtfupEawv",
        "outputId": "9e5684a5-adc3-4436-c0df-312288845e29"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+\n",
            "|avg_speed|avg_speed_clean|\n",
            "+---------+---------------+\n",
            "|NULL     |NULL           |\n",
            "|73.5     |73.5           |\n",
            "|20.0     |20.0           |\n",
            "|27.1     |27.1           |\n",
            "|59.3     |59.3           |\n",
            "|23.6     |23.6           |\n",
            "|71.1     |71.1           |\n",
            "|36.3     |36.3           |\n",
            "|49.3     |49.3           |\n",
            "|79.3     |79.3           |\n",
            "+---------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_clean_phase2 = df_time.withColumn(\"timestamp_original\", F.col(\"timestamp\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "2Vic_a6qGBo_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4 Parse timestamp into event_time with multiple formats\n",
        "ts1 = F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "ts2 = F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\")\n",
        "ts3 = F.to_timestamp(\"timestamp\", \"yyyy/MM/dd HH:mm:ss\")\n",
        "\n",
        "df_time = df_speed.withColumn(\"event_time\", F.coalesce(ts1, ts2, ts3))\n",
        "\n",
        "df_time.select(\"timestamp\", \"event_time\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "SX6C3Dg0Eato"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_time = df_speed.withColumn(\n",
        "    \"timestamp_norm\",\n",
        "    F.when(F.lower(F.col(\"timestamp\")) == \"invalid_time\", None).otherwise(F.col(\"timestamp\"))\n",
        ")\n",
        "ts1 = F.to_timestamp(\"timestamp_norm\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "ts2 = F.to_timestamp(\"timestamp_norm\", \"dd/MM/yyyy HH:mm:ss\")\n",
        "ts3 = F.to_timestamp(\"timestamp_norm\", \"yyyy/MM/dd HH:mm:ss\")\n",
        "df_time = df_time.withColumn(\"event_time\", F.coalesce(ts1, ts2, ts3)).drop(\"timestamp_norm\")\n"
      ],
      "metadata": {
        "id": "CXylFdEdEaqr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_clean_phase2 = df_time.withColumn(\"timestamp_original\", F.col(\"timestamp\"))\n"
      ],
      "metadata": {
        "id": "GHrH7apSEann"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase 3"
      ],
      "metadata": {
        "id": "D_v47WOvi8WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t1 Count invalid vehicle_count rows\n",
        "invalid_vehicle_count_raw = df_time.where(\n",
        "    F.col(\"vehicle_count_clean\").isNull()\n",
        ").count()\n",
        "\n",
        "print(\"Invalid vehicle_count rows (RAW):\", invalid_vehicle_count_raw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWReI9GFjbmV",
        "outputId": "49e95ac5-78e6-45d3-d422-35b850b29a2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid vehicle_count rows (RAW): 49873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_temp = df_speed.drop(\"event_time\")\n"
      ],
      "metadata": {
        "id": "QeJL4w7SkEft"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3.2 Count invalid timestamp rows (rows where event_time is NULL)\n",
        "invalid_timestamp_rows = df_time.where(\n",
        "    F.col(\"event_time\").isNull()\n",
        ").count()\n",
        "\n",
        "print(\"Invalid timestamp rows (RAW):\", invalid_timestamp_rows)\n"
      ],
      "metadata": {
        "id": "MXxZO6mcpmhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df_time"
      ],
      "metadata": {
        "id": "ANiaVzr9p-9t"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base = df_trimmed"
      ],
      "metadata": {
        "id": "1QfjsrcPp-uG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_clean = (\n",
        "    df_base\n",
        "    .withColumn(\"vehicle_count_lower\", F.lower(F.col(\"vehicle_count\")))\n",
        "    .withColumn(\n",
        "        \"vehicle_count_clean\",\n",
        "        F.when(\n",
        "            F.col(\"vehicle_count_lower\").isNull() |\n",
        "            (F.col(\"vehicle_count_lower\") == \"\") |\n",
        "            (F.col(\"vehicle_count_lower\") == \"invalid\"),\n",
        "            None\n",
        "        ).when(\n",
        "            F.col(\"vehicle_count_lower\").rlike(\"^[0-9]+$\"),\n",
        "            F.col(\"vehicle_count_lower\").cast(IntegerType())\n",
        "        ).otherwise(None)\n",
        "    )\n",
        "    .drop(\"vehicle_count_lower\")\n",
        "    .withColumn(\"avg_speed_lower\", F.lower(F.col(\"avg_speed\")))\n",
        "    .withColumn(\n",
        "        \"avg_speed_clean\",\n",
        "        F.when(F.col(\"avg_speed_lower\").isNull() | (F.col(\"avg_speed_lower\") == \"\"), None)\n",
        "         .otherwise(F.col(\"avg_speed_lower\").cast(\"double\"))\n",
        "    )\n",
        "    .drop(\"avg_speed_lower\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "f-G0WaUmp-qp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_clean = (\n",
        "    df_clean\n",
        "    .withColumn(\n",
        "        \"timestamp_norm\",\n",
        "        F.when(F.lower(F.col(\"timestamp\")) == \"invalid_time\", None)\n",
        "         .when(F.col(\"timestamp\") == \"\", None)\n",
        "         .otherwise(F.col(\"timestamp\"))\n",
        "    )\n",
        "    .withColumn(\"ts1\", F.try_to_timestamp(\"timestamp_norm\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "    .withColumn(\"ts2\", F.try_to_timestamp(\"timestamp_norm\", \"dd/MM/yyyy HH:mm:ss\"))\n",
        "    .withColumn(\"ts3\", F.try_to_timestamp(\"timestamp_norm\", \"yyyy/MM/dd HH:mm:ss\"))\n",
        "    .withColumn(\"ts4\", F.try_to_timestamp(\"timestamp_norm\"))\n",
        "    .withColumn(\"event_time\", F.coalesce(\"ts1\", \"ts2\", \"ts3\", \"ts4\"))\n",
        "    .drop(\"ts1\", \"ts2\", \"ts3\", \"ts4\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cdki5XUiqVq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.select(\"timestamp\", \"timestamp_norm\", \"event_time\").show(20, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "collapsed": true,
        "id": "c2hIaOFbqVnS",
        "outputId": "868325c5-1556-42c1-e5b1-6d31a70c26fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2026-01-19 09:51:06.553\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp_norm` cannot be resolved. Did you mean one of the following? [`timestamp`, `status`, `location`, `road_name`, `sensor_id`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o505.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp_norm` cannot be resolved. Did you mean one of the following? [`timestamp`, `status`, `location`, `road_name`, `sensor_id`]. SQLSTATE: 42703;\\n'Project [timestamp#110, 'timestamp_norm, 'event_time]\\n+- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_clean#253]\\n   +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_lower#252, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN cast(null as double) ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253]\\n      +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\\n         +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251]\\n            +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_lower#250, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN cast(null as int) WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) ELSE cast(null as int) END AS vehicle_count_clean#251]\\n               +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, lower(vehicle_count#107) AS vehicle_count_lower#250]\\n                  +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, trim(status#58, None) AS status#111]\\n                     +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, trim(timestamp#57, None) AS timestamp#110, status#58]\\n                        +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, trim(temperature#56, None) AS temperature#109, timestamp#57, status#58]\\n                           +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, trim(avg_speed#55, None) AS avg_speed#108, temperature#56, timestamp#57, status#58]\\n                              +- Project [sensor_id#104, location#105, road_name#106, trim(vehicle_count#54, None) AS vehicle_count#107, avg_speed#55, temperature#56, timestamp#57, status#58]\\n                                 +- Project [sensor_id#104, location#105, trim(road_name#53, None) AS road_name#106, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\\n                                    +- Project [sensor_id#104, trim(location#52, None) AS location#105, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\\n                                       +- Project [trim(sensor_id#51, None) AS sensor_id#104, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\\n                                          +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, trim(status#49, None) AS status#58]\\n                                             +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, trim(timestamp#48, None) AS timestamp#57, status#49]\\n                                                +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, trim(temperature#47, None) AS temperature#56, timestamp#48, status#49]\\n                                                   +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, trim(avg_speed#46, None) AS avg_speed#55, temperature#47, timestamp#48, status#49]\\n                                                      +- Project [sensor_id#51, location#52, road_name#53, trim(vehicle_count#45, None) AS vehicle_count#54, avg_speed#46, temperature#47, timestamp#48, status#49]\\n                                                         +- Project [sensor_id#51, location#52, trim(road_name#44, None) AS road_name#53, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\\n                                                            +- Project [sensor_id#51, trim(location#43, None) AS location#52, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\\n                                                               +- Project [trim(sensor_id#42, None) AS sensor_id#51, location#43, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\\n                                                                  +- Relation [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,temperature#47,timestamp#48,status#49] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 21 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp_norm` cannot be resolved. Did you mean one of the following? [`timestamp`, `status`, `location`, `road_name`, `sensor_id`]. SQLSTATE: 42703;\n'Project [timestamp#110, 'timestamp_norm, 'event_time]\n+- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_clean#253]\n   +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_lower#252, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN cast(null as double) ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253]\n      +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n         +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251]\n            +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_lower#250, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN cast(null as int) WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) ELSE cast(null as int) END AS vehicle_count_clean#251]\n               +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, lower(vehicle_count#107) AS vehicle_count_lower#250]\n                  +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, trim(status#58, None) AS status#111]\n                     +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, trim(timestamp#57, None) AS timestamp#110, status#58]\n                        +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, trim(temperature#56, None) AS temperature#109, timestamp#57, status#58]\n                           +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, trim(avg_speed#55, None) AS avg_speed#108, temperature#56, timestamp#57, status#58]\n                              +- Project [sensor_id#104, location#105, road_name#106, trim(vehicle_count#54, None) AS vehicle_count#107, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                 +- Project [sensor_id#104, location#105, trim(road_name#53, None) AS road_name#106, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                    +- Project [sensor_id#104, trim(location#52, None) AS location#105, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                       +- Project [trim(sensor_id#51, None) AS sensor_id#104, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                          +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, trim(status#49, None) AS status#58]\n                                             +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, trim(timestamp#48, None) AS timestamp#57, status#49]\n                                                +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, trim(temperature#47, None) AS temperature#56, timestamp#48, status#49]\n                                                   +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, trim(avg_speed#46, None) AS avg_speed#55, temperature#47, timestamp#48, status#49]\n                                                      +- Project [sensor_id#51, location#52, road_name#53, trim(vehicle_count#45, None) AS vehicle_count#54, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                         +- Project [sensor_id#51, location#52, trim(road_name#44, None) AS road_name#53, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                            +- Project [sensor_id#51, trim(location#43, None) AS location#52, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                               +- Project [trim(sensor_id#42, None) AS sensor_id#51, location#43, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                                  +- Relation [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,temperature#47,timestamp#48,status#49] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-184090763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"timestamp_norm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"event_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ColumnOrName\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp_norm` cannot be resolved. Did you mean one of the following? [`timestamp`, `status`, `location`, `road_name`, `sensor_id`]. SQLSTATE: 42703;\n'Project [timestamp#110, 'timestamp_norm, 'event_time]\n+- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_clean#253]\n   +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, avg_speed_lower#252, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN cast(null as double) ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253]\n      +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n         +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_clean#251]\n            +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, vehicle_count_lower#250, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN cast(null as int) WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) ELSE cast(null as int) END AS vehicle_count_clean#251]\n               +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, status#111, lower(vehicle_count#107) AS vehicle_count_lower#250]\n                  +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, timestamp#110, trim(status#58, None) AS status#111]\n                     +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, temperature#109, trim(timestamp#57, None) AS timestamp#110, status#58]\n                        +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, avg_speed#108, trim(temperature#56, None) AS temperature#109, timestamp#57, status#58]\n                           +- Project [sensor_id#104, location#105, road_name#106, vehicle_count#107, trim(avg_speed#55, None) AS avg_speed#108, temperature#56, timestamp#57, status#58]\n                              +- Project [sensor_id#104, location#105, road_name#106, trim(vehicle_count#54, None) AS vehicle_count#107, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                 +- Project [sensor_id#104, location#105, trim(road_name#53, None) AS road_name#106, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                    +- Project [sensor_id#104, trim(location#52, None) AS location#105, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                       +- Project [trim(sensor_id#51, None) AS sensor_id#104, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, status#58]\n                                          +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, timestamp#57, trim(status#49, None) AS status#58]\n                                             +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, temperature#56, trim(timestamp#48, None) AS timestamp#57, status#49]\n                                                +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, avg_speed#55, trim(temperature#47, None) AS temperature#56, timestamp#48, status#49]\n                                                   +- Project [sensor_id#51, location#52, road_name#53, vehicle_count#54, trim(avg_speed#46, None) AS avg_speed#55, temperature#47, timestamp#48, status#49]\n                                                      +- Project [sensor_id#51, location#52, road_name#53, trim(vehicle_count#45, None) AS vehicle_count#54, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                         +- Project [sensor_id#51, location#52, trim(road_name#44, None) AS road_name#53, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                            +- Project [sensor_id#51, trim(location#43, None) AS location#52, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                               +- Project [trim(sensor_id#42, None) AS sensor_id#51, location#43, road_name#44, vehicle_count#45, avg_speed#46, temperature#47, timestamp#48, status#49]\n                                                                  +- Relation [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,temperature#47,timestamp#48,status#49] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#t2\n",
        "invalid_timestamp_rows = df_clean.where(F.col(\"event_time\").isNull()).count()\n",
        "print(\"Invalid timestamp rows (RAW):\", invalid_timestamp_rows)\n"
      ],
      "metadata": {
        "id": "sajqjSC7tR1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t3\n",
        "df_active = df_clean.where(F.upper(F.col(\"status\")) == \"ACTIVE\")\n",
        "print(\"ACTIVE rows:\", df_active.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mio9NOQYtdPG",
        "outputId": "9c9540b6-3e85-4fba-f26d-fa975b82c566"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACTIVE rows: 475000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#t4\n",
        "active_invalid_vehicle_count = df_active.where(F.col(\"vehicle_count_clean\").isNull()).count()\n",
        "active_invalid_timestamp = df_active.where(F.col(\"event_time\").isNull()).count()\n",
        "active_missing_speed = df_active.where(F.col(\"avg_speed_clean\").isNull()).count()\n",
        "\n",
        "print(\"=== ACTIVE DATA QUALITY REPORT ===\")\n",
        "print(\"Invalid vehicle_count:\", active_invalid_vehicle_count)\n",
        "print(\"Invalid timestamp:\", active_invalid_timestamp)\n",
        "print(\"Missing avg_speed:\", active_missing_speed)\n"
      ],
      "metadata": {
        "id": "f3r_xHX0tdNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase 4"
      ],
      "metadata": {
        "id": "4AosX7TFt3cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned = df_active.select(\n",
        "    \"sensor_id\",\"location\",\"road_name\",\n",
        "    \"vehicle_count_clean\",\"avg_speed_clean\",\n",
        "    \"event_time\",\"timestamp_original\",\"status\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ELR3XcaatcCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t1 Average speed per location\n",
        "avg_speed_per_location = (\n",
        "    cleaned.groupBy(\"location\")\n",
        "           .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_location\"))\n",
        "           .orderBy(F.col(\"avg_speed_location\").asc_nulls_last())\n",
        ")\n",
        "\n",
        "avg_speed_per_location.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "NbtFtCsktb8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t2 Total vehicle count per road\n",
        "total_vehicle_per_road = (\n",
        "    cleaned.groupBy(\"road_name\")\n",
        "           .agg(F.sum(\"vehicle_count_clean\").alias(\"total_vehicle_count\"))\n",
        "           .orderBy(F.col(\"total_vehicle_count\").desc_nulls_last())\n",
        ")\n",
        "\n",
        "total_vehicle_per_road.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "DI66mpegtb48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t3 Peak traffic time per location\n",
        "hourly = (\n",
        "    cleaned.withColumn(\"event_hour\", F.date_trunc(\"hour\", F.col(\"event_time\")))\n",
        ")\n",
        "\n",
        "per_loc_hour = (\n",
        "    hourly.groupBy(\"location\", \"event_hour\")\n",
        "          .agg(F.sum(\"vehicle_count_clean\").alias(\"vehicles\"))\n",
        ")\n",
        "\n",
        "from pyspark.sql import Window\n",
        "w_loc = Window.partitionBy(\"location\").orderBy(F.col(\"vehicles\").desc_nulls_last())\n",
        "\n",
        "peak_time_per_location = (\n",
        "    per_loc_hour.withColumn(\"rank\", F.row_number().over(w_loc))\n",
        "                .where(F.col(\"rank\") == 1)\n",
        "                .drop(\"rank\")\n",
        "                .orderBy(\"location\")\n",
        ")\n",
        "\n",
        "peak_time_per_location.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "pUilwbX7tb3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t4 Roads with lowest avg speed (most congested)\n",
        "road_speed = (\n",
        "    cleaned.groupBy(\"road_name\")\n",
        "           .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_road\"))\n",
        "           .orderBy(F.col(\"avg_speed_road\").asc_nulls_last())\n",
        ")\n",
        "\n",
        "road_speed.show(50, truncate=False)\n"
      ],
      "metadata": {
        "id": "77xXdksywDid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ohase 5 task1\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "road_speed = (\n",
        "    cleaned.groupBy(\"road_name\")\n",
        "           .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_road\"))\n",
        ")\n",
        "\n",
        "w_congestion = Window.orderBy(F.col(\"avg_speed_road\").asc_nulls_last())\n",
        "road_speed_ranked = (\n",
        "    road_speed\n",
        "    .withColumn(\"congestion_rank\", F.dense_rank().over(w_congestion))\n",
        "    .orderBy(F.col(\"congestion_rank\").asc(), F.col(\"avg_speed_road\").asc_nulls_last(), F.col(\"road_name\").asc())\n",
        ")\n",
        "\n",
        "road_speed_ranked.show(50, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "kHGcAeSwzng3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#t2\n",
        "loc_road_volume = (\n",
        "    cleaned.groupBy(\"location\", \"road_name\")\n",
        "           .agg(F.sum(\"vehicle_count_clean\").alias(\"vehicles\"))\n",
        ")\n",
        "\n",
        "\n",
        "w_loc_road = Window.partitionBy(\"location\").orderBy(F.col(\"vehicles\").desc_nulls_last())\n",
        "loc_road_ranked = (\n",
        "    loc_road_volume\n",
        "    .withColumn(\"volume_rank_in_location\", F.dense_rank().over(w_loc_road))\n",
        "    .orderBy(\"location\", \"volume_rank_in_location\", F.col(\"vehicles\").desc_nulls_last(), \"road_name\")\n",
        ")\n",
        "\n",
        "loc_road_ranked.show(50, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "R2lu7L4mz_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#t3\n",
        "loc_road_speed = (\n",
        "    cleaned.groupBy(\"location\", \"road_name\")\n",
        "           .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_loc_road\"))\n",
        ")\n",
        "\n",
        "w_loc_congestion = Window.partitionBy(\"location\").orderBy(F.col(\"avg_speed_loc_road\").asc_nulls_last())\n",
        "\n",
        "top3_congested_per_location = (\n",
        "    loc_road_speed\n",
        "    .withColumn(\"congestion_rank_in_location\", F.dense_rank().over(w_loc_congestion))\n",
        "    .where(F.col(\"congestion_rank_in_location\") <= 3)\n",
        "    .orderBy(\"location\", \"congestion_rank_in_location\", \"road_name\")\n",
        ")\n",
        "\n",
        "top3_congested_per_location.show(100, truncate=False)\n"
      ],
      "metadata": {
        "id": "yEqMaiz7z_mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase 6"
      ],
      "metadata": {
        "id": "DQWisDdK2yeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#t1\n",
        "\n",
        "\n",
        "w = Window.partitionBy(\"sensor_id\").orderBy(\"event_time\")\n",
        "\n"
      ],
      "metadata": {
        "id": "coQXf4s0z_jE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t2\n",
        "count_spike = (\n",
        "    cleaned\n",
        "    .withColumn(\"prev_count\", F.lag(\"vehicle_count_clean\").over(w))\n",
        "    .withColumn(\"count_spike\", F.col(\"vehicle_count_clean\") - F.col(\"prev_count\"))\n",
        "    .where(F.col(\"count_spike\") >= 40)\n",
        ")\n",
        "\n",
        "count_spike.select(\"sensor_id\",\"event_time\",\"prev_count\",\"vehicle_count_clean\",\"count_spike\").show(20, False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvvnxZoq3EHH",
        "outputId": "22051746-8239-49bf-cc9d-a3074987e39f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+----------+-------------------+-----------+\n",
            "|sensor_id|event_time         |prev_count|vehicle_count_clean|count_spike|\n",
            "+---------+-------------------+----------+-------------------+-----------+\n",
            "|S101     |NULL               |25        |109                |84         |\n",
            "|S101     |NULL               |30        |111                |81         |\n",
            "|S101     |NULL               |36        |99                 |63         |\n",
            "|S101     |NULL               |15        |119                |104        |\n",
            "|S101     |NULL               |13        |114                |101        |\n",
            "|S101     |NULL               |25        |76                 |51         |\n",
            "|S101     |2026-01-12 09:25:30|22        |116                |94         |\n",
            "|S101     |2026-01-12 09:36:20|46        |91                 |45         |\n",
            "|S101     |2026-01-12 10:57:40|26        |92                 |66         |\n",
            "|S101     |2026-01-12 11:59:10|16        |89                 |73         |\n",
            "|S101     |2026-01-12 13:32:55|14        |85                 |71         |\n",
            "|S101     |2026-01-12 14:12:25|37        |117                |80         |\n",
            "|S101     |2026-01-12 14:34:55|54        |101                |47         |\n",
            "|S101     |2026-01-12 15:18:45|29        |89                 |60         |\n",
            "|S101     |2026-01-12 18:31:50|29        |102                |73         |\n",
            "|S101     |2026-01-12 19:09:15|52        |95                 |43         |\n",
            "|S101     |2026-01-12 21:30:20|32        |101                |69         |\n",
            "|S101     |2026-01-12 21:59:10|11        |85                 |74         |\n",
            "|S101     |2026-01-12 23:23:00|41        |120                |79         |\n",
            "|S101     |2026-01-13 00:29:20|20        |68                 |48         |\n",
            "+---------+-------------------+----------+-------------------+-----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_clean.printSchema()\n",
        "df_clean.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThjGKaJU6bBh",
        "outputId": "fed50aaf-b6dd-4d76-b0be-efee3f263342"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count: string (nullable = true)\n",
            " |-- avg_speed: string (nullable = true)\n",
            " |-- temperature: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- vehicle_count_clean: integer (nullable = true)\n",
            " |-- avg_speed_clean: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sensor_id',\n",
              " 'location',\n",
              " 'road_name',\n",
              " 'vehicle_count',\n",
              " 'avg_speed',\n",
              " 'temperature',\n",
              " 'timestamp',\n",
              " 'status',\n",
              " 'vehicle_count_clean',\n",
              " 'avg_speed_clean']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "def parse_any_timestamp(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = s.strip()\n",
        "    if s == \"\" or s.lower() == \"invalid_time\":\n",
        "        return None\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%d/%m/%Y %H:%M:%S\",\n",
        "        \"%Y/%m/%d %H:%M:%S\"\n",
        "    ]\n",
        "\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            return datetime.datetime.strptime(s, fmt)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # final fallback\n",
        "    try:\n",
        "        return datetime.datetime.fromisoformat(s)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "parse_ts_udf = F.udf(parse_any_timestamp, TimestampType())\n"
      ],
      "metadata": {
        "id": "XZfAwpw26mgG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time = df_clean.withColumn(\"event_time\", parse_ts_udf(F.col(\"timestamp\")))"
      ],
      "metadata": {
        "id": "vbojsTIZ6mdN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time.select(\"timestamp\", \"event_time\").show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3FTZef6mac",
        "outputId": "71d861f0-4655-4dd3-dc5e-80166060b999"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+\n",
            "|timestamp          |event_time         |\n",
            "+-------------------+-------------------+\n",
            "|12/01/2026 06:00:00|2026-01-12 06:00:00|\n",
            "|2026-01-12 06:00:05|2026-01-12 06:00:05|\n",
            "|2026-01-12 06:00:10|2026-01-12 06:00:10|\n",
            "|2026-01-12 06:00:15|2026-01-12 06:00:15|\n",
            "|2026-01-12 06:00:20|2026-01-12 06:00:20|\n",
            "|2026-01-12 06:00:25|2026-01-12 06:00:25|\n",
            "|2026-01-12 06:00:30|2026-01-12 06:00:30|\n",
            "|2026-01-12 06:00:35|2026-01-12 06:00:35|\n",
            "|2026-01-12 06:00:40|2026-01-12 06:00:40|\n",
            "|2026-01-12 06:00:45|2026-01-12 06:00:45|\n",
            "+-------------------+-------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned = df_time.select(\n",
        "    \"sensor_id\", \"location\", \"road_name\",\n",
        "    \"vehicle_count_clean\", \"avg_speed_clean\",\n",
        "    \"event_time\", \"timestamp\", \"status\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "DZZMXtzW6mXw"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 7"
      ],
      "metadata": {
        "id": "5bLYJ7tc7xPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T1 Partitions before any optimization\n",
        "print(\"Partitions BEFORE:\", cleaned.rdd.getNumPartitions())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guMz5pIa7ok5",
        "outputId": "feb2141b-034d-43e7-9852-007bec2deb0f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions BEFORE: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T2 Execution plan before optimization\n",
        "cleaned.explain(False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vupfe1_A6mU8",
        "outputId": "cf3e6e3b-d9d4-49a9-80d6-10d624fb2c5e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(2) Project [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN null ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253, pythonUDF0#327 AS event_time#295, timestamp#110, status#111]\n",
            "+- BatchEvalPython [parse_any_timestamp(timestamp#110)#294], [pythonUDF0#327]\n",
            "   +- *(1) Project [sensor_id#104, location#105, road_name#106, timestamp#110, status#111, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN null WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) END AS vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n",
            "      +- *(1) Project [trim(trim(sensor_id#42, None), None) AS sensor_id#104, trim(trim(location#43, None), None) AS location#105, trim(trim(road_name#44, None), None) AS road_name#106, trim(trim(avg_speed#46, None), None) AS avg_speed#108, trim(trim(timestamp#48, None), None) AS timestamp#110, trim(trim(status#49, None), None) AS status#111, lower(trim(trim(vehicle_count#45, None), None)) AS vehicle_count_lower#250]\n",
            "         +- FileScan csv [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,timestamp#48,status#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,ti...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T3 Repartition using a frequently-used column\n",
        "cleaned_part = cleaned.repartition(\"location\")\n"
      ],
      "metadata": {
        "id": "7KREX5NY6mSQ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T4 Cache and materialize\n",
        "cleaned_part = cleaned_part.cache()\n",
        "cleaned_part.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2InEPWT6mPo",
        "outputId": "208a4fb0-684c-4cb9-980f-b59ab0fcb950"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500000"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T5 Partitions after repartition(\"location\")\n",
        "print(\"Partitions AFTER:\", cleaned_part.rdd.getNumPartitions())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgvLSQ-o8lTE",
        "outputId": "c3bbe423-86a8-451d-def5-cc81361b5bbe"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions AFTER: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T6 Execution plan after optimization\n",
        "cleaned_part.explain(False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxhllf9W8lPk",
        "outputId": "fcd74c57-78bf-4353-8846-8f28729e99a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- TableCacheQueryStage 0\n",
            "      +- InMemoryTableScan [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, avg_speed_clean#253, event_time#295, timestamp#110, status#111]\n",
            "            +- InMemoryRelation [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, avg_speed_clean#253, event_time#295, timestamp#110, status#111], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                  +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                  +- == Final Plan ==\n",
            "                     ResultQueryStage 1\n",
            "                     +- ShuffleQueryStage 0\n",
            "                        +- Exchange hashpartitioning(location#105, 200), REPARTITION_BY_COL, [plan_id=622]\n",
            "                           +- *(2) Project [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN null ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253, pythonUDF0#335 AS event_time#295, timestamp#110, status#111]\n",
            "                              +- BatchEvalPython [parse_any_timestamp(timestamp#110)#294], [pythonUDF0#335]\n",
            "                                 +- *(1) Project [sensor_id#104, location#105, road_name#106, timestamp#110, status#111, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN null WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) END AS vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n",
            "                                    +- *(1) Project [trim(trim(sensor_id#42, None), None) AS sensor_id#104, trim(trim(location#43, None), None) AS location#105, trim(trim(road_name#44, None), None) AS road_name#106, trim(trim(avg_speed#46, None), None) AS avg_speed#108, trim(trim(timestamp#48, None), None) AS timestamp#110, trim(trim(status#49, None), None) AS status#111, lower(trim(trim(vehicle_count#45, None), None)) AS vehicle_count_lower#250]\n",
            "                                       +- FileScan csv [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,timestamp#48,status#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,ti...\n",
            "                  +- == Initial Plan ==\n",
            "                     Exchange hashpartitioning(location#105, 200), REPARTITION_BY_COL, [plan_id=580]\n",
            "                     +- Project [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN null ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253, pythonUDF0#335 AS event_time#295, timestamp#110, status#111]\n",
            "                        +- BatchEvalPython [parse_any_timestamp(timestamp#110)#294], [pythonUDF0#335]\n",
            "                           +- Project [sensor_id#104, location#105, road_name#106, timestamp#110, status#111, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN null WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) END AS vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n",
            "                              +- Project [trim(trim(sensor_id#42, None), None) AS sensor_id#104, trim(trim(location#43, None), None) AS location#105, trim(trim(road_name#44, None), None) AS road_name#106, trim(trim(avg_speed#46, None), None) AS avg_speed#108, trim(trim(timestamp#48, None), None) AS timestamp#110, trim(trim(status#49, None), None) AS status#111, lower(trim(trim(vehicle_count#45, None), None)) AS vehicle_count_lower#250]\n",
            "                                 +- FileScan csv [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,timestamp#48,status#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,ti...\n",
            "+- == Initial Plan ==\n",
            "   InMemoryTableScan [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, avg_speed_clean#253, event_time#295, timestamp#110, status#111]\n",
            "      +- InMemoryRelation [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, avg_speed_clean#253, event_time#295, timestamp#110, status#111], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "            +- AdaptiveSparkPlan isFinalPlan=true\n",
            "            +- == Final Plan ==\n",
            "               ResultQueryStage 1\n",
            "               +- ShuffleQueryStage 0\n",
            "                  +- Exchange hashpartitioning(location#105, 200), REPARTITION_BY_COL, [plan_id=622]\n",
            "                     +- *(2) Project [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN null ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253, pythonUDF0#335 AS event_time#295, timestamp#110, status#111]\n",
            "                        +- BatchEvalPython [parse_any_timestamp(timestamp#110)#294], [pythonUDF0#335]\n",
            "                           +- *(1) Project [sensor_id#104, location#105, road_name#106, timestamp#110, status#111, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN null WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) END AS vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n",
            "                              +- *(1) Project [trim(trim(sensor_id#42, None), None) AS sensor_id#104, trim(trim(location#43, None), None) AS location#105, trim(trim(road_name#44, None), None) AS road_name#106, trim(trim(avg_speed#46, None), None) AS avg_speed#108, trim(trim(timestamp#48, None), None) AS timestamp#110, trim(trim(status#49, None), None) AS status#111, lower(trim(trim(vehicle_count#45, None), None)) AS vehicle_count_lower#250]\n",
            "                                 +- FileScan csv [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,timestamp#48,status#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,ti...\n",
            "            +- == Initial Plan ==\n",
            "               Exchange hashpartitioning(location#105, 200), REPARTITION_BY_COL, [plan_id=580]\n",
            "               +- Project [sensor_id#104, location#105, road_name#106, vehicle_count_clean#251, CASE WHEN (isnull(avg_speed_lower#252) OR (avg_speed_lower#252 = )) THEN null ELSE cast(avg_speed_lower#252 as double) END AS avg_speed_clean#253, pythonUDF0#335 AS event_time#295, timestamp#110, status#111]\n",
            "                  +- BatchEvalPython [parse_any_timestamp(timestamp#110)#294], [pythonUDF0#335]\n",
            "                     +- Project [sensor_id#104, location#105, road_name#106, timestamp#110, status#111, CASE WHEN ((isnull(vehicle_count_lower#250) OR (vehicle_count_lower#250 = )) OR (vehicle_count_lower#250 = invalid)) THEN null WHEN RLIKE(vehicle_count_lower#250, ^[0-9]+$) THEN cast(vehicle_count_lower#250 as int) END AS vehicle_count_clean#251, lower(avg_speed#108) AS avg_speed_lower#252]\n",
            "                        +- Project [trim(trim(sensor_id#42, None), None) AS sensor_id#104, trim(trim(location#43, None), None) AS location#105, trim(trim(road_name#44, None), None) AS road_name#106, trim(trim(avg_speed#46, None), None) AS avg_speed#108, trim(trim(timestamp#48, None), None) AS timestamp#110, trim(trim(status#49, None), None) AS status#111, lower(trim(trim(vehicle_count#45, None), None)) AS vehicle_count_lower#250]\n",
            "                           +- FileScan csv [sensor_id#42,location#43,road_name#44,vehicle_count#45,avg_speed#46,timestamp#48,status#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,ti...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 8"
      ],
      "metadata": {
        "id": "OM1s4SKu83NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T1 Convert DataFrame to RDD\n",
        "rdd = cleaned_part.select(\"vehicle_count_clean\", \"location\").rdd\n"
      ],
      "metadata": {
        "id": "CDMdAoFl8lMz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T2 Total vehicle count using reduce (null-safe)\n",
        "total_vehicle_rdd = (\n",
        "    rdd.map(lambda row: row[0] if row[0] is not None else 0)\n",
        "       .reduce(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "print(\"Total vehicle count via RDD.reduce:\", total_vehicle_rdd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRlcx4Kn8lJ0",
        "outputId": "3aeed1a8-62dc-48e5-d393-d0903789b474"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vehicle count via RDD.reduce: 29255173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T3 Count records per location\n",
        "records_per_location_rdd = (\n",
        "    rdd.map(lambda row: (row[1], 1))       # (location, 1)\n",
        "       .reduceByKey(lambda a, b: a + b)   # sum counts\n",
        ")\n",
        "\n",
        "for x in records_per_location_rdd.take(10):\n",
        "    print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy3MtnZ38lG9",
        "outputId": "0171f761-f52a-4534-f9d2-b11c333cbe68"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Mumbai', 71645)\n",
            "('Delhi', 71653)\n",
            "('Kolkata', 71591)\n",
            "('Bangalore', 71285)\n",
            "('Pune', 70842)\n",
            "('Chennai', 71326)\n",
            "('Hyderabad', 71658)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T4 Convert the reduceByKey output to a DataFrame\n",
        "df_records_per_location = records_per_location_rdd.toDF([\"location\", \"record_count\"])\n",
        "\n",
        "df_records_per_location.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22llSqCd8lEN",
        "outputId": "b28643a0-2c6b-40d7-9dbf-37e9cb2c8483"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|location |record_count|\n",
            "+---------+------------+\n",
            "|Mumbai   |71645       |\n",
            "|Delhi    |71653       |\n",
            "|Kolkata  |71591       |\n",
            "|Bangalore|71285       |\n",
            "|Pune     |70842       |\n",
            "|Chennai  |71326       |\n",
            "|Hyderabad|71658       |\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T5 Filter: keep rows where count > 50\n",
        "high_count_rdd = rdd.filter(lambda row: row[0] is not None and row[0] > 50)\n",
        "\n",
        "print(\"Sample high count rows:\", high_count_rdd.take(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnwwbbGG8lBO",
        "outputId": "cbc717a8-88a9-4aa0-bf12-f70ce5820386"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample high count rows: [Row(vehicle_count_clean=87, location='Bangalore'), Row(vehicle_count_clean=68, location='Bangalore'), Row(vehicle_count_clean=74, location='Bangalore'), Row(vehicle_count_clean=74, location='Bangalore'), Row(vehicle_count_clean=100, location='Bangalore'), Row(vehicle_count_clean=85, location='Bangalore'), Row(vehicle_count_clean=82, location='Bangalore'), Row(vehicle_count_clean=59, location='Bangalore'), Row(vehicle_count_clean=119, location='Bangalore'), Row(vehicle_count_clean=117, location='Bangalore')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 9"
      ],
      "metadata": {
        "id": "ynNAu78l965L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 Sort roads by lowest avg speed (congestion metric)\n",
        "roads_by_congestion = (\n",
        "    cleaned.groupBy(\"road_name\")\n",
        "           .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_road\"))\n",
        "           .orderBy(F.col(\"avg_speed_road\").asc_nulls_last())\n",
        ")\n",
        "\n",
        "roads_by_congestion.show(20, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFG0Qx1i-HAd",
        "outputId": "a18d434e-0870-40fa-e59f-f3a2f2688b9a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------------------+\n",
            "|road_name      |avg_speed_road    |\n",
            "+---------------+------------------+\n",
            "|EM Bypass      |47.29303486907817 |\n",
            "|Link Road      |47.368898290300194|\n",
            "|FC Road        |47.37180375306915 |\n",
            "|Nagar Rd       |47.40516559782131 |\n",
            "|Whitefield Rd  |47.414129958840526|\n",
            "|MG Road        |47.421224489796046|\n",
            "|Howrah Rd      |47.4229107618311  |\n",
            "|Gachibowli Rd  |47.42927475774559 |\n",
            "|GST Road       |47.43261706083269 |\n",
            "|Hitech City Rd |47.483875274559495|\n",
            "|University Rd  |47.49609046283315 |\n",
            "|Outer Ring Rd  |47.49839820874945 |\n",
            "|Eastern Express|47.51013993541459 |\n",
            "|Western Express|47.51883443020463 |\n",
            "|Ring Road      |47.537259072755596|\n",
            "|OMR            |47.54642794453223 |\n",
            "|Park Street    |47.603439327967635|\n",
            "|NH48           |47.62648557609277 |\n",
            "|Madhapur Rd    |47.634299244070036|\n",
            "|Janpath        |47.65433455433439 |\n",
            "+---------------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2 Create Sets A and B\n",
        "\n",
        "# Set A\n",
        "set_A = cleaned.where(F.col(\"avg_speed_clean\") < 25).select(\"road_name\").distinct()\n",
        "\n",
        "# Set B\n",
        "set_B = cleaned.where(F.col(\"vehicle_count_clean\") > 60).select(\"road_name\").distinct()\n"
      ],
      "metadata": {
        "id": "3mBrUZVZ-G9A"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3 Intersection\n",
        "roads_both = set_A.join(set_B, \"road_name\", \"inner\")\n",
        "\n",
        "roads_both.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcrRwWcl-G5q",
        "outputId": "9b243eb9-869a-46bf-958d-020f4a7f1775"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|road_name      |\n",
            "+---------------+\n",
            "|University Rd  |\n",
            "|Western Express|\n",
            "|Eastern Express|\n",
            "|FC Road        |\n",
            "|Whitefield Rd  |\n",
            "|Link Road      |\n",
            "|Outer Ring Rd  |\n",
            "|Gachibowli Rd  |\n",
            "|Janpath        |\n",
            "|Hitech City Rd |\n",
            "|GST Road       |\n",
            "|OMR            |\n",
            "|NH48           |\n",
            "|Ring Road      |\n",
            "|Mount Road     |\n",
            "|Howrah Rd      |\n",
            "|Park Street    |\n",
            "|Madhapur Rd    |\n",
            "|EM Bypass      |\n",
            "|MG Road        |\n",
            "+---------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 10"
      ],
      "metadata": {
        "id": "FZfb1sWc-ybW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OUT_DIR = \"/content/traffic_outputs_phase10\"\n"
      ],
      "metadata": {
        "id": "v84XIE6i-G2n"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 Parquet (partitioned by location)\n",
        "(\n",
        "    cleaned\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"location\")\n",
        "    .parquet(f\"{OUT_DIR}/cleaned_parquet_by_location\")\n",
        ")\n",
        "print(\" Wrote partitioned Parquet:\", f\"{OUT_DIR}/cleaned_parquet_by_location\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfvJUe1N-Gzr",
        "outputId": "346b5d2e-8cda-4f07-c7f9-d778abaab904"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wrote partitioned Parquet: /content/traffic_outputs_phase10/cleaned_parquet_by_location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F, Window\n",
        "\n",
        "if \"road_speed_ranked\" not in locals():\n",
        "    road_speed = (\n",
        "        cleaned.groupBy(\"road_name\")\n",
        "               .agg(F.avg(\"avg_speed_clean\").alias(\"avg_speed_road\"))\n",
        "    )\n",
        "    w_congestion = Window.orderBy(F.col(\"avg_speed_road\").asc_nulls_last())\n",
        "    road_speed_ranked = (\n",
        "        road_speed\n",
        "        .withColumn(\"congestion_rank\", F.dense_rank().over(w_congestion))\n",
        "    )\n",
        "\n",
        "#2\n",
        "(\n",
        "    road_speed_ranked\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .orc(f\"{OUT_DIR}/congestion_analytics_orc\")\n",
        ")\n",
        "print(\" Wrote ORC analytics:\", f\"{OUT_DIR}/congestion_analytics_orc\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPY8UvUz-Gwv",
        "outputId": "2051d25f-1f8d-478b-d766-e23fb27bc117"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wrote ORC analytics: /content/traffic_outputs_phase10/congestion_analytics_orc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# T3\n",
        "read_cleaned = spark.read.parquet(f\"{OUT_DIR}/cleaned_parquet_by_location\")\n",
        "read_orc     = spark.read.orc(f\"{OUT_DIR}/congestion_analytics_orc\")\n",
        "\n",
        "print(\"Cleaned Parquet row count:\", read_cleaned.count(), \"| Original cleaned count:\", cleaned.count())\n",
        "print(\"ORC analytics row count:\", read_orc.count(), \"| Original analytics count:\", road_speed_ranked.count())\n",
        "\n",
        "print(\"\\n-- Cleaned Parquet schema --\")\n",
        "read_cleaned.printSchema()\n",
        "\n",
        "print(\"\\n-- ORC analytics schema --\")\n",
        "read_orc.printSchema()\n",
        "\n",
        "print(\"\\nSample (cleaned Parquet):\")\n",
        "read_cleaned.show(10, truncate=False)\n",
        "\n",
        "print(\"\\nSample (ORC analytics):\")\n",
        "read_orc.orderBy(F.col(\"congestion_rank\").asc_nulls_last()).show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMC0X9rq_LJt",
        "outputId": "af788017-8481-447b-a646-a4f78c110455"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Parquet row count: 500000 | Original cleaned count: 500000\n",
            "ORC analytics row count: 21 | Original analytics count: 21\n",
            "\n",
            "-- Cleaned Parquet schema --\n",
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count_clean: integer (nullable = true)\n",
            " |-- avg_speed_clean: double (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "\n",
            "-- ORC analytics schema --\n",
            "root\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- avg_speed_road: double (nullable = true)\n",
            " |-- congestion_rank: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample (cleaned Parquet):\n",
            "+---------+---------------+-------------------+---------------+-------------------+-------------------+------+--------+\n",
            "|sensor_id|road_name      |vehicle_count_clean|avg_speed_clean|event_time         |timestamp          |status|location|\n",
            "+---------+---------------+-------------------+---------------+-------------------+-------------------+------+--------+\n",
            "|S252     |Western Express|115                |59.3           |2026-01-12 06:00:20|2026-01-12 06:00:20|ACTIVE|Mumbai  |\n",
            "|S205     |Link Road      |56                 |51.2           |2026-01-12 06:01:35|2026-01-12 06:01:35|ACTIVE|Mumbai  |\n",
            "|S232     |Western Express|105                |19.7           |2026-01-12 06:02:10|2026-01-12 06:02:10|ACTIVE|Mumbai  |\n",
            "|S133     |Link Road      |47                 |43.2           |2026-01-12 06:02:20|2026-01-12 06:02:20|ACTIVE|Mumbai  |\n",
            "|S250     |Western Express|64                 |23.8           |2026-01-12 06:02:55|2026-01-12 06:02:55|ACTIVE|Mumbai  |\n",
            "|S288     |Eastern Express|78                 |26.1           |2026-01-12 06:03:05|2026/01/12 06:03:05|ACTIVE|Mumbai  |\n",
            "|S163     |Link Road      |46                 |78.5           |2026-01-12 06:03:10|2026-01-12 06:03:10|ACTIVE|Mumbai  |\n",
            "|S139     |Western Express|62                 |77.7           |2026-01-12 06:03:35|2026-01-12 06:03:35|ACTIVE|Mumbai  |\n",
            "|S168     |Western Express|18                 |41.8           |2026-01-12 06:03:45|2026-01-12 06:03:45|ACTIVE|Mumbai  |\n",
            "|S213     |Western Express|NULL               |28.0           |2026-01-12 06:03:50|2026-01-12 06:03:50|ACTIVE|Mumbai  |\n",
            "+---------+---------------+-------------------+---------------+-------------------+-------------------+------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Sample (ORC analytics):\n",
            "+--------------+------------------+---------------+\n",
            "|road_name     |avg_speed_road    |congestion_rank|\n",
            "+--------------+------------------+---------------+\n",
            "|EM Bypass     |47.29303486907817 |1              |\n",
            "|Link Road     |47.368898290300194|2              |\n",
            "|FC Road       |47.37180375306915 |3              |\n",
            "|Nagar Rd      |47.40516559782131 |4              |\n",
            "|Whitefield Rd |47.414129958840526|5              |\n",
            "|MG Road       |47.421224489796046|6              |\n",
            "|Howrah Rd     |47.4229107618311  |7              |\n",
            "|Gachibowli Rd |47.42927475774559 |8              |\n",
            "|GST Road      |47.43261706083269 |9              |\n",
            "|Hitech City Rd|47.483875274559495|10             |\n",
            "+--------------+------------------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    }
  ]
}