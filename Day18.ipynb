{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LQ36gA9KoUhW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder \\\n",
        "    .appName(\"Read CSV example\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AMMnUpqepJPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = [\n",
        "    (\"U001\",\"Abhishek\",28,\"Hyderabad\",50000),\n",
        "    (\"U002\",\"Neha\",32,\"Delhi\",62000),\n",
        "    (\"U003\",\"Ravi\",25,\"Bangalore\",45000),\n",
        "    (\"U004\",\"Pooja\",29,\"Mumbai\",58000)\n",
        "]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DbT9VGJWo17a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import(\n",
        "    StructType,\n",
        "    StructField,\n",
        "    StringType,\n",
        "    IntegerType,LongType\n",
        ")"
      ],
      "metadata": {
        "id": "A6ZEd2RRpRhS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema = StructType([\n",
        "    StructField(\"user_id\",StringType(),nullable=False),\n",
        "    StructField(\"user_name\",StringType(),nullable=True),\n",
        "    StructField(\"age\",IntegerType(),nullable=True),\n",
        "    StructField(\"city\",StringType(),nullable=True),\n",
        "    StructField(\"salary\",LongType(),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "6XXeWO64pc4p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users=spark.createDataFrame(raw_data,schema=user_schema)"
      ],
      "metadata": {
        "id": "3ZVrB0rdp2cZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMpiS1GWqDdi",
        "outputId": "93308dc6-4746-49d0-bd49-a077ac8c9547"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_users.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQXb4gHPqHUl",
        "outputId": "2bb03b1d-3a2a-460a-a82c-a5d78818e3eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---+---------+------+\n",
            "|user_id|user_name|age|     city|salary|\n",
            "+-------+---------+---+---------+------+\n",
            "|   U001| Abhishek| 28|Hyderabad| 50000|\n",
            "|   U002|     Neha| 32|    Delhi| 62000|\n",
            "|   U003|     Ravi| 25|Bangalore| 45000|\n",
            "|   U004|    Pooja| 29|   Mumbai| 58000|\n",
            "+-------+---------+---+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data2=[\n",
        "    (\"P001\",\"Abhishek\",\"Thirty\",\"Hyderabad\",50000)\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "KmlDgIdfqLoh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users=spark.createDataFrame(raw_data2,schema=user_schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "cdegeqCoqWaX",
        "outputId": "67860949-cdc3-4d90-f956-253b2a83431e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field age: IntegerType() can not accept object 'Thirty' in type <class 'str'>.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-361272533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_users\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1597\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             )\n\u001b[0;32m-> 1599\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             rdd, struct = self._createFromLocal(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mno_type_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1615\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2929\u001b[0m                     )\n\u001b[1;32m   2930\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2931\u001b[0;31m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2932\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dict__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2834\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mverify_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m             \u001b[0massert_acceptable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2836\u001b[0;31m             \u001b[0mverify_acceptable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2837\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2147483648\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m             \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2147483647\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_acceptable_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m                 raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   2745\u001b[0m                     \u001b[0merrorClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m                     messageParameters={\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field age: IntegerType() can not accept object 'Thirty' in type <class 'str'>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import ArrayType"
      ],
      "metadata": {
        "id": "0eQdwpLjrDUN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interest_data = [\n",
        "    (\"U001\",[\"AI\",\"ML\",\"Cloud\"]),\n",
        "    (\"U002\",[\"Testing\",\"Automation\"]),\n",
        "    (\"U003\",[\"Data Engineering\",\"Spark\",\"Kafka\"]),\n",
        "    (\"U004\",[\"UI/UX\"])\n",
        "]"
      ],
      "metadata": {
        "id": "kaHSSe_UrMiF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interest_schema = StructType([\n",
        "    StructField(\"user_id\",StringType(),nullable=False),\n",
        "    StructField(\"interests\",ArrayType(StringType()),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "J9UgvXFArQw8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_interests=spark.createDataFrame(interest_data,schema=interest_schema)\n",
        "df_interests.printSchema()\n",
        "df_interests.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF4x1Ux0rV_M",
        "outputId": "a55bc54c-00fa-499c-e521-d88f08c60836"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------+--------------------------------+\n",
            "|user_id|interests                       |\n",
            "+-------+--------------------------------+\n",
            "|U001   |[AI, ML, Cloud]                 |\n",
            "|U002   |[Testing, Automation]           |\n",
            "|U003   |[Data Engineering, Spark, Kafka]|\n",
            "|U004   |[UI/UX]                         |\n",
            "+-------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_interests.select(\n",
        "    \"user_id\",\n",
        "    explode(\"interests\").alias(\"interest\")\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfAtzZsFrbrT",
        "outputId": "b298a715-8e73-43b2-c20f-259758ad703a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+\n",
            "|user_id|        interest|\n",
            "+-------+----------------+\n",
            "|   U001|              AI|\n",
            "|   U001|              ML|\n",
            "|   U001|           Cloud|\n",
            "|   U002|         Testing|\n",
            "|   U002|      Automation|\n",
            "|   U003|Data Engineering|\n",
            "|   U003|           Spark|\n",
            "|   U003|           Kafka|\n",
            "|   U004|           UI/UX|\n",
            "+-------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import MapType"
      ],
      "metadata": {
        "id": "B_FuxWySs1No"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_data = [\n",
        "    (\"U001\",{\"mobile\":120,\"laptop\":300}),\n",
        "    (\"U002\",{\"tablet\":80}),\n",
        "    (\"U003\",{\"mobile\":200,\"desktop\":400}),\n",
        "    (\"U004\",{\"laptop\":250})\n",
        "]\n",
        ""
      ],
      "metadata": {
        "id": "tLNI3BtatCE3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_schema=StructType([\n",
        "    StructField(\"user_id\",StringType(),nullable=False),\n",
        "    StructField(\"device_usage\",MapType(StringType(),LongType()),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "1kCbwE0ctIG2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_devices=spark.createDataFrame(device_data,schema=device_schema)\n",
        "df_devices.printSchema()\n",
        "df_devices.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-yoIgA2tQ7X",
        "outputId": "2f2e1d58-64ad-4108-cd36-306eb94278bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: long (valueContainsNull = true)\n",
            "\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usage                   |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{tablet -> 80}                 |\n",
            "|U003   |{mobile -> 200, desktop -> 400}|\n",
            "|U004   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nested_data = [\n",
        "    (\"U001\",(\"Hyderabad\",\"Telangana\",500081)),\n",
        "    (\"U002\",(\"Delhi\",\"Delhi\",110001)),\n",
        "    (\"U003\",(\"Bangalore\",\"Karnataka\",560001))\n",
        "]"
      ],
      "metadata": {
        "id": "-CemByvtuqKi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address_schema=StructType([\n",
        "    StructField(\"city\",StringType(),True),\n",
        "    StructField(\"state\",StringType(),True),\n",
        "    StructField(\"pincode\",StringType(),True)\n",
        "])\n",
        "profile_schema=StructType([\n",
        "    StructField(\"user_id\",StringType(),False),\n",
        "    StructField(\"address\",address_schema,True)\n",
        "])"
      ],
      "metadata": {
        "id": "D-8BCPvXu4_k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_profiles=spark.createDataFrame(nested_data,profile_schema)\n",
        "df_profiles.printSchema()\n",
        "df_profiles.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs-jKJMlvUxU",
        "outputId": "1db242e6-5106-44f8-9350-ca207b3bee23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- pincode: string (nullable = true)\n",
            "\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "+-------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_profiles.select(\n",
        "    \"user_id\",\n",
        "    \"address.city\",\n",
        "    \"address.state\",\n",
        "\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3vwWxYv_mh",
        "outputId": "10736011-7c99-44f0-de3c-1662a2e69974"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+\n",
            "|user_id|     city|    state|\n",
            "+-------+---------+---------+\n",
            "|   U001|Hyderabad|Telangana|\n",
            "|   U002|    Delhi|    Delhi|\n",
            "|   U003|Bangalore|Karnataka|\n",
            "+-------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df_users.withColumn(\n",
        "    \"salary_int\",\n",
        "    col(\"salary\").cast(\"int\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_kMnSQWwLQW",
        "outputId": "c4723366-0596-435d-a155-1ebe03d3fef3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[user_id: string, user_name: string, age: int, city: string, salary: bigint, salary_int: int]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "df_orders.withColumn(\n",
        "    \"order_date\",\n",
        "    to_date(\"order_date\",\"yyyy-MM-dd\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sIAGQ2Zswv5N",
        "outputId": "4f13cffd-6c94-46e7-b2e4-ad6d8caa92fa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_orders' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4210798079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_orders.withColumn(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_orders' is not defined"
          ]
        }
      ]
    }
  ]
}