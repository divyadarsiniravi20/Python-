{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder \\\n",
        "    .appName(\"Read CSV example\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QcIN8bonhVgS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.1\n",
        "\n",
        "Create a transformation pipeline that:\n",
        "Filters only Completed rides\n",
        "Selects ride_id , city , distance_km\n",
        "Tasks:\n",
        "Do not trigger any action\n",
        "Explain whether Spark executed anything"
      ],
      "metadata": {
        "id": "XWFJ3JY5jwZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-SF1pKi-ghSx"
      },
      "outputs": [],
      "source": [
        "\n",
        "rides_data = [\n",
        "    (\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "    (\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "    (\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "    (\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "    (\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "    (\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "    (\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "    (\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "    (\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "    (\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "rides_cols = [\"ride_id\",\"user_id\",\"city\",\"distance_km\",\"duration_seconds\",\"status\"]\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)\n",
        "\n",
        "completed_rides_df = rides_df.filter(rides_df.status == \"Completed\")\n",
        "selected_cols_df = completed_rides_df.select(\"ride_id\", \"city\", \"distance_km\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.2\n",
        "\n",
        "Trigger a single action on the pipeline.\n",
        "Tasks:\n",
        "\n",
        "Identify which line caused execution\n",
        "Explain why previous lines did not execute"
      ],
      "metadata": {
        "id": "UMR053hTj5vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_data = [\n",
        "    (\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "    (\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "    (\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "    (\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "    (\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "    (\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "    (\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "    (\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "    (\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "    (\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "rides_cols = [\"ride_id\",\"user_id\",\"city\",\"distance_km\",\"duration_seconds\",\"status\"]\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)\n",
        "\n",
        "completed_rides_df = rides_df.filter(rides_df.status == \"Completed\")\n",
        "selected_cols_df = completed_rides_df.select(\"ride_id\", \"city\", \"distance_km\")\n",
        "\n",
        "result_count = selected_cols_df.count()\n",
        "\n",
        "print(f\"Number of completed rides (with selected columns): {result_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2etlBkqiHEF",
        "outputId": "1dd08084-9ad9-4d84-b180-88f106ef4d57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of completed rides (with selected columns): 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE SET 2 — DAG & LINEAGE\n",
        "\n",
        "Exercise 2.1\n",
        "\n",
        "Create a transformation chain with:\n",
        "Multiple filters\n",
        "A column selection\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Logical plan\n",
        "Optimized logical plan\n",
        "Physical plan"
      ],
      "metadata": {
        "id": "2d3aLw17kVlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "pipeline_df = (\n",
        "    rides_df\n",
        "        .filter(col(\"status\") == \"Completed\")      # Filter 1\n",
        "        .filter(col(\"distance_km\") > 10)           # Filter 2\n",
        "        .filter(col(\"duration_seconds\") >= 200)    # Filter 3\n",
        "        .select(\"ride_id\", \"city\", \"distance_km\")  # Column selection\n",
        ")\n",
        "pipeline_df.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J17kuxOCkLQ4",
        "outputId": "6f1983b7-bc3b-444e-9655-699158a5e92d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km]\n",
            "+- Filter (duration_seconds#23L >= cast(200 as bigint))\n",
            "   +- Filter (distance_km#22 > cast(10 as double))\n",
            "      +- Filter (status#24 = Completed)\n",
            "         +- LogicalRDD [ride_id#19, user_id#20, city#21, distance_km#22, duration_seconds#23L, status#24], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double\n",
            "Project [ride_id#19, city#21, distance_km#22]\n",
            "+- Filter (duration_seconds#23L >= cast(200 as bigint))\n",
            "   +- Filter (distance_km#22 > cast(10 as double))\n",
            "      +- Filter (status#24 = Completed)\n",
            "         +- LogicalRDD [ride_id#19, user_id#20, city#21, distance_km#22, duration_seconds#23L, status#24], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#19, city#21, distance_km#22]\n",
            "+- Filter (((isnotnull(status#24) AND isnotnull(distance_km#22)) AND isnotnull(duration_seconds#23L)) AND ((status#24 = Completed) AND ((distance_km#22 > 10.0) AND (duration_seconds#23L >= 200))))\n",
            "   +- LogicalRDD [ride_id#19, user_id#20, city#21, distance_km#22, duration_seconds#23L, status#24], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [ride_id#19, city#21, distance_km#22]\n",
            "+- *(1) Filter (((isnotnull(status#24) AND isnotnull(distance_km#22)) AND isnotnull(duration_seconds#23L)) AND ((status#24 = Completed) AND ((distance_km#22 > 10.0) AND (duration_seconds#23L >= 200))))\n",
            "   +- *(1) Scan ExistingRDD[ride_id#19,user_id#20,city#21,distance_km#22,duration_seconds#23L,status#24]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.2\n",
        "\n",
        "Reorder transformations (filter after join vs before join).\n",
        "Tasks:\n",
        "Compare DAGs\n",
        "Identify which plan is more efficient and why"
      ],
      "metadata": {
        "id": "fRDHUkOCnD2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rides_data = [\n",
        "    (\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "    (\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "    (\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "    (\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "    (\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "    (\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "    (\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "    (\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "    (\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "    (\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "rides_cols = [\"ride_id\",\"user_id\",\"city\",\"distance_km\",\"duration_seconds\",\"status\"]\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)\n",
        "\n",
        "surge_data = [\n",
        "    (\"Hyderabad\",1.2),\n",
        "    (\"Delhi\",1.5),\n",
        "    (\"Mumbai\",1.8),\n",
        "    (\"Bangalore\",1.3)\n",
        "]\n",
        "surge_cols = [\"city\",\"surge_multiplier\"]\n",
        "surge_df = spark.createDataFrame(surge_data, surge_cols)\n"
      ],
      "metadata": {
        "id": "xoeD7kIRkbyu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "pipeline_A = (\n",
        "    rides_df\n",
        "        .filter(col(\"distance_km\") > 10)            # Filter first\n",
        "        .join(surge_df, on=\"city\", how=\"inner\")     # Then join\n",
        "        .select(\"ride_id\", \"city\", \"distance_km\", \"surge_multiplier\")\n",
        ")\n",
        "\n",
        "print(\"\\n=== Pipeline A (Filter BEFORE Join) — explain(True) ===\")\n",
        "pipeline_A.explain(True)\n",
        "\n",
        "\n",
        "pipeline_B = (\n",
        "    rides_df\n",
        "        .join(surge_df, on=\"city\", how=\"inner\")     # Join first\n",
        "        .filter(col(\"distance_km\") > 10)            # Then filter\n",
        "        .select(\"ride_id\", \"city\", \"distance_km\", \"surge_multiplier\")\n",
        ")\n",
        "\n",
        "print(\"\\n=== Pipeline B (Join BEFORE Filter) — explain(True) ===\")\n",
        "pipeline_B.explain(True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1CZ6UsGm2yX",
        "outputId": "f0644edf-397c-41d1-eafb-b2477b0079f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Pipeline A (Filter BEFORE Join) — explain(True) ===\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km, 'surge_multiplier]\n",
            "+- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "   +- Join Inner, (city#34 = city#38)\n",
            "      :- Filter (distance_km#35 > cast(10 as double))\n",
            "      :  +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double, surge_multiplier: double\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "   +- Join Inner, (city#34 = city#38)\n",
            "      :- Filter (distance_km#35 > cast(10 as double))\n",
            "      :  +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Join Inner, (city#34 = city#38)\n",
            "   :- Project [ride_id#32, city#34, distance_km#35]\n",
            "   :  +- Filter ((isnotnull(distance_km#35) AND (distance_km#35 > 10.0)) AND isnotnull(city#34))\n",
            "   :     +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "   +- Filter isnotnull(city#38)\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "   +- SortMergeJoin [city#34], [city#38], Inner\n",
            "      :- Sort [city#34 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#34, 200), ENSURE_REQUIREMENTS, [plan_id=133]\n",
            "      :     +- Project [ride_id#32, city#34, distance_km#35]\n",
            "      :        +- Filter ((isnotnull(distance_km#35) AND (distance_km#35 > 10.0)) AND isnotnull(city#34))\n",
            "      :           +- Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "      +- Sort [city#38 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#38, 200), ENSURE_REQUIREMENTS, [plan_id=134]\n",
            "            +- Filter isnotnull(city#38)\n",
            "               +- Scan ExistingRDD[city#38,surge_multiplier#39]\n",
            "\n",
            "\n",
            "=== Pipeline B (Join BEFORE Filter) — explain(True) ===\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km, 'surge_multiplier]\n",
            "+- Filter (distance_km#35 > cast(10 as double))\n",
            "   +- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "      +- Join Inner, (city#34 = city#38)\n",
            "         :- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "         +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double, surge_multiplier: double\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Filter (distance_km#35 > cast(10 as double))\n",
            "   +- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "      +- Join Inner, (city#34 = city#38)\n",
            "         :- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "         +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Join Inner, (city#34 = city#38)\n",
            "   :- Project [ride_id#32, city#34, distance_km#35]\n",
            "   :  +- Filter ((isnotnull(distance_km#35) AND (distance_km#35 > 10.0)) AND isnotnull(city#34))\n",
            "   :     +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "   +- Filter isnotnull(city#38)\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "   +- SortMergeJoin [city#34], [city#38], Inner\n",
            "      :- Sort [city#34 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#34, 200), ENSURE_REQUIREMENTS, [plan_id=168]\n",
            "      :     +- Project [ride_id#32, city#34, distance_km#35]\n",
            "      :        +- Filter ((isnotnull(distance_km#35) AND (distance_km#35 > 10.0)) AND isnotnull(city#34))\n",
            "      :           +- Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "      +- Sort [city#38 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#38, 200), ENSURE_REQUIREMENTS, [plan_id=169]\n",
            "            +- Filter isnotnull(city#38)\n",
            "               +- Scan ExistingRDD[city#38,surge_multiplier#39]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE SET 3 — PARTITIONS & SHUFFLE\n",
        "\n",
        "Exercise 3.1\n",
        "\n",
        "Check the number of partitions of rides_df .\n",
        "Tasks:\n",
        "Repartition into 4 partitions\n",
        "\n",
        "Coalesce into 1 partition\n",
        "Observe number of output files when writing to Parquet"
      ],
      "metadata": {
        "id": "zKxS-MDwnxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial partitions:\", rides_df.rdd.getNumPartitions())\n",
        "rides_4 = rides_df.repartition(4)\n",
        "print(\"After repartition(4):\", rides_4.rdd.getNumPartitions())\n",
        "rides_1 = rides_4.coalesce(1)\n",
        "print(\"After coalesce(1):\", rides_1.rdd.getNumPartitions())\n",
        "rides_df.write.mode(\"overwrite\").parquet(\"/tmp/initial\")\n",
        "rides_4.write.mode(\"overwrite\").parquet(\"/tmp/repartition4\")\n",
        "rides_1.write.mode(\"overwrite\").parquet(\"/tmp/coalesce1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yGbVHNCn2qG",
        "outputId": "3de27d77-f5ba-46c9-ebce-8c987e5703fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial partitions: 2\n",
            "After repartition(4): 4\n",
            "After coalesce(1): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3.2\n",
        "\n",
        "Repartition rides by city .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify whether a shuffle is introduced"
      ],
      "metadata": {
        "id": "EOQ4GMqlofaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_by_city = rides_df.repartition(\"city\")\n",
        "print(\"Partitions after repartition by city:\", rides_by_city.rdd.getNumPartitions())\n",
        "rides_by_city.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6tvdljoo5v",
        "outputId": "82d48e0f-ef1a-46ab-ed4f-98e54bb9190c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions after repartition by city: 1\n",
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, user_id: string, city: string, distance_km: double, duration_seconds: bigint, status: string\n",
            "RepartitionByExpression [city#34]\n",
            "+- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#34]\n",
            "+- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 0\n",
            "         +- Exchange hashpartitioning(city#34, 200), REPARTITION_BY_COL, [plan_id=349]\n",
            "            +- *(1) Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "+- == Initial Plan ==\n",
            "   Exchange hashpartitioning(city#34, 200), REPARTITION_BY_COL, [plan_id=342]\n",
            "   +- Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE SET 4 — JOIN WITHOUT BROADCAST (BAD\n",
        "DAG)\n",
        "\n",
        "Exercise 4.1\n",
        "\n",
        "Join rides_df with surge_df on city without using broadcast.\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "Exchange operators\n",
        "Sort operations\n",
        "Stage boundaries"
      ],
      "metadata": {
        "id": "k9Wwv7jXo_xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "join_df = rides_df.join(surge_df, on=\"city\", how=\"inner\")\n",
        "join_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbErTdSKpGOx",
        "outputId": "5cac3fae-2b47-4177-b277-b482c726a994"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "+- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "+- Join Inner, (city#34 = city#38)\n",
            "   :- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "   +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "+- Join Inner, (city#34 = city#38)\n",
            "   :- Filter isnotnull(city#34)\n",
            "   :  +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "   +- Filter isnotnull(city#38)\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "   +- SortMergeJoin [city#34], [city#38], Inner\n",
            "      :- Sort [city#34 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#34, 200), ENSURE_REQUIREMENTS, [plan_id=382]\n",
            "      :     +- Filter isnotnull(city#34)\n",
            "      :        +- Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "      +- Sort [city#38 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#38, 200), ENSURE_REQUIREMENTS, [plan_id=383]\n",
            "            +- Filter isnotnull(city#38)\n",
            "               +- Scan ExistingRDD[city#38,surge_multiplier#39]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4.2\n",
        "\n",
        "Apply a filter ( distance_km > 10 ) before the join.\n",
        "Tasks:\n",
        "Observe whether shuffle is removed\n",
        "Explain why or why not"
      ],
      "metadata": {
        "id": "JzJCv8aLqZAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "filtered_join_df = (\n",
        "    rides_df\n",
        "               .filter(col(\"distance_km\") > 10)      # selective filter\n",
        "        .join(surge_df, on=\"city\", how=\"inner\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "LtiPSN4jtHoR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5.1\n",
        "\n",
        "Apply a broadcast hint to surge_df .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "BroadcastExchange\n",
        "Disappearance of shuffles"
      ],
      "metadata": {
        "id": "ZAbDrlVxtgkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, broadcast\n",
        "\n",
        "broadcast_join_df = (\n",
        "    rides_df\n",
        "        .filter(col(\"status\") == \"Completed\")\n",
        "        .join(broadcast(surge_df), on=\"city\", how=\"inner\")\n",
        "        .select(\"ride_id\", \"city\", \"distance_km\", \"surge_multiplier\")\n",
        ")\n",
        "broadcast_join_df.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoeCLBIst1bm",
        "outputId": "3aeb475f-8757-48f7-e56d-784a71ce7e28"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km, 'surge_multiplier]\n",
            "+- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "   +- Join Inner, (city#34 = city#38)\n",
            "      :- Filter (status#37 = Completed)\n",
            "      :  +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double, surge_multiplier: double\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Project [city#34, ride_id#32, user_id#33, distance_km#35, duration_seconds#36L, status#37, surge_multiplier#39]\n",
            "   +- Join Inner, (city#34 = city#38)\n",
            "      :- Filter (status#37 = Completed)\n",
            "      :  +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "+- Join Inner, (city#34 = city#38), rightHint=(strategy=broadcast)\n",
            "   :- Project [ride_id#32, city#34, distance_km#35]\n",
            "   :  +- Filter ((isnotnull(status#37) AND (status#37 = Completed)) AND isnotnull(city#34))\n",
            "   :     +- LogicalRDD [ride_id#32, user_id#33, city#34, distance_km#35, duration_seconds#36L, status#37], false\n",
            "   +- Filter isnotnull(city#38)\n",
            "      +- LogicalRDD [city#38, surge_multiplier#39], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [ride_id#32, city#34, distance_km#35, surge_multiplier#39]\n",
            "   +- BroadcastHashJoin [city#34], [city#38], Inner, BuildRight, false\n",
            "      :- Project [ride_id#32, city#34, distance_km#35]\n",
            "      :  +- Filter ((isnotnull(status#37) AND (status#37 = Completed)) AND isnotnull(city#34))\n",
            "      :     +- Scan ExistingRDD[ride_id#32,user_id#33,city#34,distance_km#35,duration_seconds#36L,status#37]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=416]\n",
            "         +- Filter isnotnull(city#38)\n",
            "            +- Scan ExistingRDD[city#38,surge_multiplier#39]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5.2\n",
        "\n",
        "Compare physical plans from:\n",
        "Exercise 4.1\n",
        "Exercise 5.1\n",
        "Tasks:\n",
        "List operators that disappeared\n",
        "Explain performance impact"
      ],
      "metadata": {
        "id": "t2tvER0tuIE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ISriTPqruj5q"
      }
    }
  ]
}