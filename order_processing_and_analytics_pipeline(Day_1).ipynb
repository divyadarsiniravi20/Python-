{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 1"
      ],
      "metadata": {
        "id": "IfOg2dAMjoLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"OrderPipeline\").getOrCreate()\n",
        "\n",
        "# Load CSV without schema inference\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(\"orders.csv\")\n",
        "\n",
        "# Print schema\n",
        "df.printSchema()\n",
        "\n",
        "# Count records\n",
        "print(\"Total Records:\", df.count())\n",
        "\n",
        "# Show sample rows\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVcNBupFcSS3",
        "outputId": "5249ca47-30a2-4cf7-cdbd-981aca6c4c2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "Total Records: 300000\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B8bxKm6pIcnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2\n"
      ],
      "metadata": {
        "id": "c9gP6-H3jk2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#phase 2 task 1\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Proper case = trim → lower → initcap\n",
        "def proper_case(col):\n",
        "    return F.initcap(F.lower(F.trim(F.col(col))))\n",
        "\n",
        "df2 = (df\n",
        "       .withColumn(\"city_std\",     proper_case(\"city\"))\n",
        "       .withColumn(\"category_std\", proper_case(\"category\"))\n",
        "       .withColumn(\"product_std\",  proper_case(\"product\"))\n",
        "       .withColumn(\"status_std\",   proper_case(\"status\")))\n",
        "\n",
        "df2.select(\"city\",\"city_std\",\"category\",\"category_std\",\"product\",\"product_std\",\"status\",\"status_std\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HCVUOR7f2cZ",
        "outputId": "fb5c48ef-9f33-4a25-ee8b-c0559ba70f68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-----------+------------+-----------+-----------+---------+----------+\n",
            "|city       |city_std |category   |category_std|product    |product_std|status   |status_std|\n",
            "+-----------+---------+-----------+------------+-----------+-----------+---------+----------+\n",
            "| hyderabad |Hyderabad| grocery   |Grocery     |Oil        |Oil        |Cancelled|Cancelled |\n",
            "|Pune       |Pune     |Grocery    |Grocery     |Sugar      |Sugar      |Completed|Completed |\n",
            "|Pune       |Pune     |Electronics|Electronics |Mobile     |Mobile     |Completed|Completed |\n",
            "|Bangalore  |Bangalore|Electronics|Electronics |Laptop     |Laptop     |Completed|Completed |\n",
            "|Pune       |Pune     |Home       |Home        |AirPurifier|Airpurifier|Completed|Completed |\n",
            "|Delhi      |Delhi    |Fashion    |Fashion     |Jeans      |Jeans      |Completed|Completed |\n",
            "|Delhi      |Delhi    |Grocery    |Grocery     |Sugar      |Sugar      |Completed|Completed |\n",
            "|Pune       |Pune     |Grocery    |Grocery     |Rice       |Rice       |Completed|Completed |\n",
            "|Bangalore  |Bangalore|Fashion    |Fashion     |Jeans      |Jeans      |Completed|Completed |\n",
            "|Kolkata    |Kolkata  |Electronics|Electronics |Laptop     |Laptop     |Completed|Completed |\n",
            "+-----------+---------+-----------+------------+-----------+-----------+---------+----------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean amount column t3\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "amount_no_commas = F.regexp_replace(F.trim(F.col(\"amount\")), \",\", \"\")\n",
        "\n",
        "df3 = (df2\n",
        "       .withColumn(\"amount_clean\",\n",
        "                   F.when(amount_no_commas.rlike(r\"^[0-9]+$\"),\n",
        "                          amount_no_commas.cast(IntegerType()))\n",
        "                    .otherwise(F.lit(None))))\n",
        "\n",
        "df3.select(\"amount\",\"amount_clean\").show(15, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPV6cIodgnSn",
        "outputId": "5567ba74-23f0-483c-8894-e8eb08273f44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|amount |amount_clean|\n",
            "+-------+------------+\n",
            "|invalid|NULL        |\n",
            "|35430  |35430       |\n",
            "|65358  |65358       |\n",
            "|5558   |5558        |\n",
            "|33659  |33659       |\n",
            "|8521   |8521        |\n",
            "|42383  |42383       |\n",
            "|45362  |45362       |\n",
            "|10563  |10563       |\n",
            "|63715  |63715       |\n",
            "|66576  |66576       |\n",
            "|50318  |50318       |\n",
            "|84768  |84768       |\n",
            "|79121  |79121       |\n",
            "|79469  |79469       |\n",
            "+-------+------------+\n",
            "only showing top 15 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Clean the order_date column: t4\n",
        "from pyspark.sql import functions as F, types as T\n",
        "from datetime import datetime\n",
        "\n",
        "@F.udf(T.StringType())\n",
        "def normalize_date_any(s):\n",
        "    if not s:\n",
        "        return None\n",
        "    s = s.strip().replace(\".\", \"/\")\n",
        "    for fmt in (\"%Y-%m-%d\", \"%d/%m/%Y\", \"%Y/%m/%d\"):\n",
        "        try:\n",
        "            return datetime.strptime(s, fmt).strftime(\"%Y-%m-%d\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None  # couldn't parse\n",
        "\n",
        "df4 = (df3\n",
        "       .withColumn(\"order_date_norm\", normalize_date_any(\"order_date\"))\n",
        "       .withColumn(\"order_date_clean\", F.to_date(\"order_date_norm\", \"yyyy-MM-dd\"))\n",
        "       .drop(\"order_date_norm\"))\n",
        "\n",
        "df4.select(\"order_id\",\"order_date\",\"order_date_clean\").show(15, truncate=False)\n",
        "print(\"Rows with invalid date:\", df4.filter(F.col(\"order_date_clean\").isNull()).count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZK9hkObhksg",
        "outputId": "7a459680-f420-4262-d709-b11ced8f0a1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----------------+\n",
            "|order_id   |order_date|order_date_clean|\n",
            "+-----------+----------+----------------+\n",
            "|ORD00000000|01/01/2024|2024-01-01      |\n",
            "|ORD00000001|2024-01-02|2024-01-02      |\n",
            "|ORD00000002|2024-01-03|2024-01-03      |\n",
            "|ORD00000003|2024-01-04|2024-01-04      |\n",
            "|ORD00000004|2024-01-05|2024-01-05      |\n",
            "|ORD00000005|2024-01-06|2024-01-06      |\n",
            "|ORD00000006|2024-01-07|2024-01-07      |\n",
            "|ORD00000007|2024-01-08|2024-01-08      |\n",
            "|ORD00000008|2024-01-09|2024-01-09      |\n",
            "|ORD00000009|2024-01-10|2024-01-10      |\n",
            "|ORD00000010|2024-01-11|2024-01-11      |\n",
            "|ORD00000011|12/01/2024|2024-01-12      |\n",
            "|ORD00000012|2024-01-13|2024-01-13      |\n",
            "|ORD00000013|2024/01/14|2024-01-14      |\n",
            "|ORD00000014|2024-01-15|2024-01-15      |\n",
            "+-----------+----------+----------------+\n",
            "only showing top 15 rows\n",
            "Rows with invalid date: 2595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The original columns must remain for auditing. t5\n",
        "\n",
        "df4.select(\n",
        "    \"order_id\",\"customer_id\",\n",
        "    \"city\",\"city_std\",\n",
        "    \"category\",\"category_std\",\n",
        "    \"product\",\"product_std\",\n",
        "    \"amount\",\"amount_clean\",\n",
        "    \"order_date\",\"order_date_clean\",\n",
        "    \"status\",\"status_std\"\n",
        ").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFq3CJ8ohkpG",
        "outputId": "93b8aab4-94bd-42e9-e511-2f16071a8e46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+---------+-----------+------------+-----------+-----------+-------+------------+----------+----------------+---------+----------+\n",
            "|order_id   |customer_id|city       |city_std |category   |category_std|product    |product_std|amount |amount_clean|order_date|order_date_clean|status   |status_std|\n",
            "+-----------+-----------+-----------+---------+-----------+------------+-----------+-----------+-------+------------+----------+----------------+---------+----------+\n",
            "|ORD00000000|C000000    | hyderabad |Hyderabad| grocery   |Grocery     |Oil        |Oil        |invalid|NULL        |01/01/2024|2024-01-01      |Cancelled|Cancelled |\n",
            "|ORD00000001|C000001    |Pune       |Pune     |Grocery    |Grocery     |Sugar      |Sugar      |35430  |35430       |2024-01-02|2024-01-02      |Completed|Completed |\n",
            "|ORD00000002|C000002    |Pune       |Pune     |Electronics|Electronics |Mobile     |Mobile     |65358  |65358       |2024-01-03|2024-01-03      |Completed|Completed |\n",
            "|ORD00000003|C000003    |Bangalore  |Bangalore|Electronics|Electronics |Laptop     |Laptop     |5558   |5558        |2024-01-04|2024-01-04      |Completed|Completed |\n",
            "|ORD00000004|C000004    |Pune       |Pune     |Home       |Home        |AirPurifier|Airpurifier|33659  |33659       |2024-01-05|2024-01-05      |Completed|Completed |\n",
            "|ORD00000005|C000005    |Delhi      |Delhi    |Fashion    |Fashion     |Jeans      |Jeans      |8521   |8521        |2024-01-06|2024-01-06      |Completed|Completed |\n",
            "|ORD00000006|C000006    |Delhi      |Delhi    |Grocery    |Grocery     |Sugar      |Sugar      |42383  |42383       |2024-01-07|2024-01-07      |Completed|Completed |\n",
            "|ORD00000007|C000007    |Pune       |Pune     |Grocery    |Grocery     |Rice       |Rice       |45362  |45362       |2024-01-08|2024-01-08      |Completed|Completed |\n",
            "|ORD00000008|C000008    |Bangalore  |Bangalore|Fashion    |Fashion     |Jeans      |Jeans      |10563  |10563       |2024-01-09|2024-01-09      |Completed|Completed |\n",
            "|ORD00000009|C000009    |Kolkata    |Kolkata  |Electronics|Electronics |Laptop     |Laptop     |63715  |63715       |2024-01-10|2024-01-10      |Completed|Completed |\n",
            "+-----------+-----------+-----------+---------+-----------+------------+-----------+-----------+-------+------------+----------+----------------+---------+----------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3"
      ],
      "metadata": {
        "id": "DEqFvgirkonj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task1 Count how many records had invalid amounts\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "invalid_amount_count = df4.filter(F.col(\"amount_clean\").isNull()).count()\n",
        "print(\"Invalid/empty amount rows:\", invalid_amount_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUXtaQw6hkmt",
        "outputId": "47d318dd-5f22-4e64-fb53-d826f646b69b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid/empty amount rows: 25164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2: Count how many records had invalid dates\n",
        "invalid_date_count = df4.filter(F.col(\"order_date_clean\").isNull()).count()\n",
        "print(\"Invalid/empty date rows:\", invalid_date_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcDLNDd0hkj2",
        "outputId": "04352ccc-4950-478e-fe96-9744f48491cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid/empty date rows: 2595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 3:Identify duplicate order_id values\n",
        "dups = (df4.groupBy(\"order_id\")\n",
        "           .agg(F.count(\"*\").alias(\"cnt\"))\n",
        "           .filter(F.col(\"cnt\") > 1))\n",
        "\n",
        "dup_groups = dups.count()\n",
        "print(\"Duplicate order_id groups:\", dup_groups)\n",
        "dups.show(20, truncate=False)  # preview a few if any\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRIG73NohkZJ",
        "outputId": "840a8b92-e03d-47bc-d76c-c4a9c7572cf1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate order_id groups: 0\n",
            "+--------+---+\n",
            "|order_id|cnt|\n",
            "+--------+---+\n",
            "+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove duplicates using order_id. TASK 4\n",
        "\n",
        "df5_simple_dedup = df4.dropDuplicates([\"order_id\"])\n",
        "print(\"Rows after simple dedup:\", df5_simple_dedup.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYw0q_u9hkMg",
        "outputId": "96062001-84a6-49f3-eff5-57d40958b6fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows after simple dedup: 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.partitionBy(\"order_id\").orderBy(F.col(\"order_date_clean\").desc_nulls_last())\n",
        "\n",
        "df5_latest = (df4\n",
        "              .withColumn(\"rn\", F.row_number().over(w))\n",
        "              .filter(F.col(\"rn\") == 1)\n",
        "              .drop(\"rn\"))\n",
        "\n",
        "print(\"Rows after 'keep latest by date' dedup:\", df5_latest.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxDCnWSTl-2k",
        "outputId": "0956a9b2-32bf-4a55-d257-a1033ac0e983"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows after 'keep latest by date' dedup: 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5: Filter only records with status = \"Completed\"\n",
        "df6_completed = df5_latest.filter(F.lower(F.col(\"status_std\")) == \"completed\")\n",
        "print(\"Rows with status='Completed':\", df6_completed.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN2qcNXwl1bM",
        "outputId": "8828de2d-10ab-4aa9-fdf1-c2ef361308d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with status='Completed': 285000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 6: Record row counts at every stage\n",
        "raw_count        = df.count()\n",
        "clean_count      = df4.count()\n",
        "dedup_count      = df5_latest.count()\n",
        "completed_count  = df6_completed.count()\n",
        "\n",
        "print(\"=== Row Count Checkpoints ===\")\n",
        "print(f\"Raw:        {raw_count}\")\n",
        "print(f\"Cleaned:    {clean_count}\")\n",
        "print(f\"Dedup:      {dedup_count}\")\n",
        "print(f\"Completed:  {completed_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "karTrJ05l1Xw",
        "outputId": "b3203f28-968f-4e1c-bb01-524f7a541f54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Row Count Checkpoints ===\n",
            "Raw:        300000\n",
            "Cleaned:    300000\n",
            "Dedup:      300000\n",
            "Completed:  285000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 4"
      ],
      "metadata": {
        "id": "LgSy45NGnIo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 4 – Task 1\n",
        "num_partitions = df6_completed.rdd.getNumPartitions()\n",
        "print(\"Current partitions:\", num_partitions)\n",
        "\n",
        "\n",
        "print(\"spark.sql.shuffle.partitions =\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiu8E9TZl1VU",
        "outputId": "7afe3b82-903c-40f5-8906-89c9cbe11b34"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current partitions: 2\n",
            "spark.sql.shuffle.partitions = 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 4 – Task 2\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "rev_by_city = (df6_completed\n",
        "               .groupBy(\"city_std\")\n",
        "               .agg(F.sum(\"amount_clean\").alias(\"total_revenue\"))\n",
        "              )\n",
        "\n",
        "rev_by_city.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPoO8EShl1ST",
        "outputId": "f2dc5e71-8333-4d9e-c514-84ae1fe24bc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|city_std |total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|1628527093   |\n",
            "|Chennai  |1629865247   |\n",
            "|Mumbai   |1625518096   |\n",
            "|Kolkata  |1624300497   |\n",
            "|Pune     |1646196535   |\n",
            "|Delhi    |1639639916   |\n",
            "|Hyderabad|1642443340   |\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 4 – Task 3\n",
        "rev_by_city.explain(True)\n"
      ],
      "metadata": {
        "id": "yDiNRFKil1Pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7d1204-2f4e-489b-8add-bcf3aac21b03"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city_std], ['city_std, 'sum('amount_clean) AS total_revenue#445]\n",
            "+- Filter (lower(status_std#74) = completed)\n",
            "   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "      +- Filter (rn#318 = 1)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city_std: string, total_revenue: bigint\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#445L]\n",
            "+- Filter (lower(status_std#74) = completed)\n",
            "   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "      +- Filter (rn#318 = 1)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#445L]\n",
            "+- Project [city_std#71, amount_clean#104]\n",
            "   +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "      +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "         +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1\n",
            "            +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#479, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "               +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#479]\n",
            "                  +- Project [order_id#17, city#19, amount#22, order_date#23, status#24]\n",
            "                     +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city_std#71], functions=[sum(amount_clean#104)], output=[city_std#71, total_revenue#445L])\n",
            "   +- Exchange hashpartitioning(city_std#71, 200), ENSURE_REQUIREMENTS, [plan_id=1650]\n",
            "      +- HashAggregate(keys=[city_std#71], functions=[partial_sum(amount_clean#104)], output=[city_std#71, sum#465L])\n",
            "         +- Project [city_std#71, amount_clean#104]\n",
            "            +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Final\n",
            "                     +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                        +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=1642]\n",
            "                           +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Partial\n",
            "                              +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                                 +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#479, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "                                    +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#479]\n",
            "                                       +- FileScan csv [order_id#17,city#19,amount#22,order_date#23,status#24] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,order_date:string,status:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Look for 'Exchange' nodes in the physical plan above — those are the shuffle boundaries.\")#TASK 4"
      ],
      "metadata": {
        "id": "1-Y5STKvl1M2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc17e11-d4ed-4de5-f273-801612cd00fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Look for 'Exchange' nodes in the physical plan above — those are the shuffle boundaries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 4 – Task 5\n",
        "\n",
        "target_partitions = 16\n",
        "\n",
        "df6_by_city = df6_completed.repartition(target_partitions, \"city_std\")\n",
        "print(\"Repartitioned partitions:\", df6_by_city.rdd.getNumPartitions())\n",
        "\n",
        "rev_by_city_after = (df6_by_city\n",
        "                     .groupBy(\"city_std\")\n",
        "                     .agg(F.sum(\"amount_clean\").alias(\"total_revenue\")))\n",
        "\n",
        "rev_by_city_after.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GbG9nvoJ1XT",
        "outputId": "8cd0844b-5004-4622-c2c1-3f6062b80e1a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repartitioned partitions: 16\n",
            "+---------+-------------+\n",
            "|city_std |total_revenue|\n",
            "+---------+-------------+\n",
            "|Delhi    |1639639916   |\n",
            "|Chennai  |1629865247   |\n",
            "|Kolkata  |1624300497   |\n",
            "|Hyderabad|1642443340   |\n",
            "|Pune     |1646196535   |\n",
            "|Bangalore|1628527093   |\n",
            "|Mumbai   |1625518096   |\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 4 – Task 6\n",
        "print(\"=== Plan BEFORE repartition ===\")\n",
        "rev_by_city.explain(True)\n",
        "\n",
        "print(\"\\n=== Plan AFTER repartition(city_std) ===\")\n",
        "rev_by_city_after.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GGNHm1FJ1T6",
        "outputId": "234243d3-4664-4731-872d-2cfd4b5a6bcf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Plan BEFORE repartition ===\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city_std], ['city_std, 'sum('amount_clean) AS total_revenue#445]\n",
            "+- Filter (lower(status_std#74) = completed)\n",
            "   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "      +- Filter (rn#318 = 1)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city_std: string, total_revenue: bigint\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#445L]\n",
            "+- Filter (lower(status_std#74) = completed)\n",
            "   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "      +- Filter (rn#318 = 1)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#445L]\n",
            "+- Project [city_std#71, amount_clean#104]\n",
            "   +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "      +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "         +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1\n",
            "            +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#479, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "               +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#479]\n",
            "                  +- Project [order_id#17, city#19, amount#22, order_date#23, status#24]\n",
            "                     +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city_std#71], functions=[sum(amount_clean#104)], output=[city_std#71, total_revenue#445L])\n",
            "   +- Exchange hashpartitioning(city_std#71, 200), ENSURE_REQUIREMENTS, [plan_id=1650]\n",
            "      +- HashAggregate(keys=[city_std#71], functions=[partial_sum(amount_clean#104)], output=[city_std#71, sum#465L])\n",
            "         +- Project [city_std#71, amount_clean#104]\n",
            "            +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Final\n",
            "                     +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                        +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=1642]\n",
            "                           +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Partial\n",
            "                              +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                                 +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#479, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "                                    +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#479]\n",
            "                                       +- FileScan csv [order_id#17,city#19,amount#22,order_date#23,status#24] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,order_date:string,status:string>\n",
            "\n",
            "\n",
            "=== Plan AFTER repartition(city_std) ===\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city_std], ['city_std, 'sum('amount_clean) AS total_revenue#489]\n",
            "+- RepartitionByExpression [city_std#71], 16\n",
            "   +- Filter (lower(status_std#74) = completed)\n",
            "      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "         +- Filter (rn#318 = 1)\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "                  +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                                +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city_std: string, total_revenue: bigint\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#489L]\n",
            "+- RepartitionByExpression [city_std#71], 16\n",
            "   +- Filter (lower(status_std#74) = completed)\n",
            "      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "         +- Filter (rn#318 = 1)\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            "                  +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            "                                                +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city_std#71], [city_std#71, sum(amount_clean#104) AS total_revenue#489L]\n",
            "+- RepartitionByExpression [city_std#71], 16\n",
            "   +- Project [city_std#71, amount_clean#104]\n",
            "      +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "         +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "            +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1\n",
            "               +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#520, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "                  +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#520]\n",
            "                     +- Project [order_id#17, city#19, amount#22, order_date#23, status#24]\n",
            "                        +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city_std#71], functions=[sum(amount_clean#104)], output=[city_std#71, total_revenue#489L])\n",
            "   +- HashAggregate(keys=[city_std#71], functions=[partial_sum(amount_clean#104)], output=[city_std#71, sum#509L])\n",
            "      +- Exchange hashpartitioning(city_std#71, 16), REPARTITION_BY_NUM, [plan_id=2047]\n",
            "         +- Project [city_std#71, amount_clean#104]\n",
            "            +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "               +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "                  +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Final\n",
            "                     +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                        +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=2041]\n",
            "                           +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Partial\n",
            "                              +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "                                 +- Project [order_id#17, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#520, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "                                    +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#520]\n",
            "                                       +- FileScan csv [order_id#17,city#19,amount#22,order_date#23,status#24] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,order_date:string,status:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 5"
      ],
      "metadata": {
        "id": "GUBVFCAHKwN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 — Total revenue per city\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "rev_city = (df6_completed\n",
        "            .groupBy(\"city_std\")\n",
        "            .agg(F.sum(\"amount_clean\").alias(\"total_revenue\"))\n",
        "            .orderBy(F.col(\"total_revenue\").desc()))\n",
        "\n",
        "rev_city.show(20, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMLFczN6J1RN",
        "outputId": "b84d68e4-f664-4e8e-8013-ed2eb03410f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|city_std |total_revenue|\n",
            "+---------+-------------+\n",
            "|Pune     |1646196535   |\n",
            "|Hyderabad|1642443340   |\n",
            "|Delhi    |1639639916   |\n",
            "|Chennai  |1629865247   |\n",
            "|Bangalore|1628527093   |\n",
            "|Mumbai   |1625518096   |\n",
            "|Kolkata  |1624300497   |\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Total revenue per category\n",
        "rev_category = (df6_completed\n",
        "                .groupBy(\"category_std\")\n",
        "                .agg(F.sum(\"amount_clean\").alias(\"total_revenue\"))\n",
        "                .orderBy(F.col(\"total_revenue\").desc()))\n",
        "\n",
        "rev_category.show(20, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWDUb00WJ1Ol",
        "outputId": "65962f15-05e3-4c32-ae7a-8563912b4832"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|category_std|total_revenue|\n",
            "+------------+-------------+\n",
            "|Home        |2868467576   |\n",
            "|Electronics |2867568870   |\n",
            "|Grocery     |2866272106   |\n",
            "|Fashion     |2834182172   |\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 — Average order value per city\n",
        "avg_order_city = (df6_completed\n",
        "                  .groupBy(\"city_std\")\n",
        "                  .agg(F.avg(\"amount_clean\").alias(\"avg_order_value\"))\n",
        "                  .orderBy(F.col(\"avg_order_value\").desc()))\n",
        "\n",
        "avg_order_city.show(20, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DObtMycUK3KA",
        "outputId": "b81a843d-8196-438e-c002-1052a1e2d276"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "|city_std |avg_order_value   |\n",
            "+---------+------------------+\n",
            "|Bangalore|44098.867908689645|\n",
            "|Pune     |43930.204013556424|\n",
            "|Delhi    |43817.20780331374 |\n",
            "|Mumbai   |43723.75651612556 |\n",
            "|Kolkata  |43709.816662630175|\n",
            "|Hyderabad|43708.74045293664 |\n",
            "|Chennai  |43628.27900315863 |\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 — Top 10 products by revenue\n",
        "\n",
        "top_products = (df6_completed\n",
        "                .groupBy(\"product_std\")\n",
        "                .agg(F.sum(\"amount_clean\").alias(\"total_revenue\"))\n",
        "                .orderBy(F.col(\"total_revenue\").desc())\n",
        "                .limit(10))\n",
        "\n",
        "top_products.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cumxecBNK3GV",
        "outputId": "a50037d4-f009-4dc3-9d10-e9fea5eae630"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|product_std|total_revenue|\n",
            "+-----------+-------------+\n",
            "|Oil        |963572869    |\n",
            "|Laptop     |962496295    |\n",
            "|Tablet     |960719999    |\n",
            "|Vacuum     |959149427    |\n",
            "|Mixer      |957140026    |\n",
            "|Rice       |954494237    |\n",
            "|Airpurifier|952178123    |\n",
            "|Jeans      |951286127    |\n",
            "|Sugar      |948205000    |\n",
            "|Shoes      |946799102    |\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5 — Cities sorted by revenue (descending)\n",
        "sorted_cities = rev_city\n",
        "sorted_cities.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh4wKbTkK3Cj",
        "outputId": "13a9c3c2-099c-49f7-cb49-45194b885908"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|city_std |total_revenue|\n",
            "+---------+-------------+\n",
            "|Pune     |1646196535   |\n",
            "|Hyderabad|1642443340   |\n",
            "|Delhi    |1639639916   |\n",
            "|Chennai  |1629865247   |\n",
            "|Bangalore|1628527093   |\n",
            "|Mumbai   |1625518096   |\n",
            "|Kolkata  |1624300497   |\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 6"
      ],
      "metadata": {
        "id": "2aI1vPN9Lwa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n"
      ],
      "metadata": {
        "id": "fH1dKwxfK2_U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 — Rank cities by revenue\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "city_revenue = (df6_completed\n",
        "                .groupBy(\"city_std\")\n",
        "                .agg(F.sum(\"amount_clean\").alias(\"total_revenue\")))\n",
        "\n",
        "w1 = Window.orderBy(F.col(\"total_revenue\").desc())\n",
        "\n",
        "ranked_cities = city_revenue.withColumn(\"city_rank\", F.dense_rank().over(w1))\n",
        "\n",
        "ranked_cities.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfmIZJovJ1MS",
        "outputId": "4fd2a7ef-4e84-4e9b-b673-d50bffa0ac94"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+---------+\n",
            "|city_std |total_revenue|city_rank|\n",
            "+---------+-------------+---------+\n",
            "|Pune     |1646196535   |1        |\n",
            "|Hyderabad|1642443340   |2        |\n",
            "|Delhi    |1639639916   |3        |\n",
            "|Chennai  |1629865247   |4        |\n",
            "|Bangalore|1628527093   |5        |\n",
            "|Mumbai   |1625518096   |6        |\n",
            "|Kolkata  |1624300497   |7        |\n",
            "+---------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Rank products inside each category by revenue\n",
        "product_rev = (df6_completed\n",
        "               .groupBy(\"category_std\", \"product_std\")\n",
        "               .agg(F.sum(\"amount_clean\").alias(\"total_revenue\")))\n",
        "\n",
        "w2 = Window.partitionBy(\"category_std\").orderBy(F.col(\"total_revenue\").desc())\n",
        "\n",
        "ranked_products = product_rev.withColumn(\"product_rank\", F.dense_rank().over(w2))\n",
        "\n",
        "ranked_products.show(50, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH89_3ZHJ1Jy",
        "outputId": "e45d0316-2b70-4f7d-a099-9436249a0e63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-------------+------------+\n",
            "|category_std|product_std|total_revenue|product_rank|\n",
            "+------------+-----------+-------------+------------+\n",
            "|Electronics |Laptop     |962496295    |1           |\n",
            "|Electronics |Tablet     |960719999    |2           |\n",
            "|Electronics |Mobile     |944352576    |3           |\n",
            "|Fashion     |Jeans      |951286127    |1           |\n",
            "|Fashion     |Shoes      |946799102    |2           |\n",
            "|Fashion     |Tshirt     |936096943    |3           |\n",
            "|Grocery     |Oil        |963572869    |1           |\n",
            "|Grocery     |Rice       |954494237    |2           |\n",
            "|Grocery     |Sugar      |948205000    |3           |\n",
            "|Home        |Vacuum     |959149427    |1           |\n",
            "|Home        |Mixer      |957140026    |2           |\n",
            "|Home        |Airpurifier|952178123    |3           |\n",
            "+------------+-----------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 — Find the top product for every category\n",
        "top_product_each_category = ranked_products.filter(F.col(\"product_rank\") == 1)\n",
        "\n",
        "top_product_each_category.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "133f0WbULzeA",
        "outputId": "b4400825-f656-4e5c-dcf0-8d629bcf8a6e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-------------+------------+\n",
            "|category_std|product_std|total_revenue|product_rank|\n",
            "+------------+-----------+-------------+------------+\n",
            "|Electronics |Laptop     |962496295    |1           |\n",
            "|Fashion     |Jeans      |951286127    |1           |\n",
            "|Grocery     |Oil        |963572869    |1           |\n",
            "|Home        |Vacuum     |959149427    |1           |\n",
            "+------------+-----------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 — Identify the top 3 performing cities\n",
        "top_3_cities = ranked_cities.filter(F.col(\"city_rank\") <= 3)\n",
        "\n",
        "top_3_cities.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG1vp0q-Lzap",
        "outputId": "d168e1d0-f3ce-4eb6-eae0-4259f94856b2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+---------+\n",
            "|city_std |total_revenue|city_rank|\n",
            "+---------+-------------+---------+\n",
            "|Pune     |1646196535   |1        |\n",
            "|Hyderabad|1642443340   |2        |\n",
            "|Delhi    |1639639916   |3        |\n",
            "+---------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 7"
      ],
      "metadata": {
        "id": "hGNNpx_lMj2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 — Create the small lookup table (city → region)\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Small lookup table\n",
        "lookup_data = [\n",
        "    Row(city=\"Delhi\",      region=\"North\"),\n",
        "    Row(city=\"Mumbai\",     region=\"West\"),\n",
        "    Row(city=\"Bangalore\",  region=\"South\"),\n",
        "    Row(city=\"Hyderabad\",  region=\"South\"),\n",
        "    Row(city=\"Pune\",       region=\"West\"),\n",
        "    Row(city=\"Chennai\",    region=\"South\"),\n",
        "    Row(city=\"Kolkata\",    region=\"East\")\n",
        "]\n",
        "\n",
        "lookup_df = spark.createDataFrame(lookup_data)\n",
        "\n",
        "lookup_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktKY4cD2LzX6",
        "outputId": "c2d1b429-a641-4128-ff4b-bb112f45dabd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|    Delhi| North|\n",
            "|   Mumbai|  West|\n",
            "|Bangalore| South|\n",
            "|Hyderabad| South|\n",
            "|     Pune|  West|\n",
            "|  Chennai| South|\n",
            "|  Kolkata|  East|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Apply broadcast join\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "df7_joined = (df6_completed\n",
        "              .join(\n",
        "                  broadcast(lookup_df),\n",
        "                  df6_completed.city_std == lookup_df.city,\n",
        "                  \"left\"\n",
        "              ))\n",
        "\n",
        "df7_joined.select(\"order_id\",\"city_std\",\"region\",\"amount_clean\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUo8X23rLzVW",
        "outputId": "5d979203-b339-4adf-c19f-5d996176c6e0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------+------------+\n",
            "|order_id   |city_std |region|amount_clean|\n",
            "+-----------+---------+------+------------+\n",
            "|ORD00000001|Pune     |West  |35430       |\n",
            "|ORD00000007|Pune     |West  |45362       |\n",
            "|ORD00000008|Bangalore|South |10563       |\n",
            "|ORD00000010|Bangalore|South |66576       |\n",
            "|ORD00000011|Kolkata  |East  |50318       |\n",
            "|ORD00000012|Bangalore|South |84768       |\n",
            "|ORD00000014|Mumbai   |West  |79469       |\n",
            "|ORD00000015|Pune     |West  |81018       |\n",
            "|ORD00000017|Bangalore|South |69582       |\n",
            "|ORD00000019|Mumbai   |West  |NULL        |\n",
            "+-----------+---------+------+------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df7_joined.explain(True)\n",
        "#Task 3 — Verify that BroadcastHashJoin is used"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLvXwVizLzSF",
        "outputId": "6af48237-61be-46da-dabc-45fc18311120"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Join LeftOuter, (city_std#71 = city#870)\n",
            ":- Filter (lower(status_std#74) = completed)\n",
            ":  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":     +- Filter (rn#318 = 1)\n",
            ":        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            ":           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            ":              +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            ":                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            ":                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            ":                                   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            ":                                      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            ":                                         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            ":                                            +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#870, region#871], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_std: string, category_std: string, product_std: string, status_std: string, amount_clean: int, order_date_clean: date, city: string, region: string\n",
            "Join LeftOuter, (city_std#71 = city#870)\n",
            ":- Filter (lower(status_std#74) = completed)\n",
            ":  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":     +- Filter (rn#318 = 1)\n",
            ":        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318]\n",
            ":           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115, rn#318, rn#318]\n",
            ":              +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_norm#114, to_date(order_date_norm#114, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#115]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, normalize_date_any(order_date#23)#113 AS order_date_norm#114]\n",
            ":                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) ELSE cast(null as int) END AS amount_clean#104]\n",
            ":                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74]\n",
            ":                                   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73]\n",
            ":                                      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72]\n",
            ":                                         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71]\n",
            ":                                            +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#870, region#871], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join LeftOuter, (city_std#71 = city#870), rightHint=(strategy=broadcast)\n",
            ":- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            ":  +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            ":     +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            ":        +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1\n",
            ":           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#898, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            ":              +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#898]\n",
            ":                 +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- Filter isnotnull(city#870)\n",
            "   +- LogicalRDD [city#870, region#871], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [city_std#71], [city#870], LeftOuter, BuildRight, false\n",
            "   :- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_std#71, category_std#72, product_std#73, status_std#74, amount_clean#104, order_date_clean#115]\n",
            "   :  +- Filter (isnotnull(status_std#74) AND ((rn#318 = 1) AND (lower(status_std#74) = completed)))\n",
            "   :     +- Window [row_number() windowspecdefinition(order_id#17, order_date_clean#115 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#318], [order_id#17], [order_date_clean#115 DESC NULLS LAST]\n",
            "   :        +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Final\n",
            "   :           +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "   :              +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=4493]\n",
            "   :                 +- WindowGroupLimit [order_id#17], [order_date_clean#115 DESC NULLS LAST], row_number(), 1, Partial\n",
            "   :                    +- Sort [order_id#17 ASC NULLS FIRST, order_date_clean#115 DESC NULLS LAST], false, 0\n",
            "   :                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, initcap(lower(trim(city#19, None))) AS city_std#71, initcap(lower(trim(category#20, None))) AS category_std#72, initcap(lower(trim(product#21, None))) AS product_std#73, initcap(lower(trim(status#24, None))) AS status_std#74, CASE WHEN RLIKE(regexp_replace(trim(amount#22, None), ,, , 1), ^[0-9]+$) THEN cast(regexp_replace(trim(amount#22, None), ,, , 1) as int) END AS amount_clean#104, cast(gettimestamp(pythonUDF0#898, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS order_date_clean#115]\n",
            "   :                          +- BatchEvalPython [normalize_date_any(order_date#23)#113], [pythonUDF0#898]\n",
            "   :                             +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=4500]\n",
            "      +- Filter isnotnull(city#870)\n",
            "         +- Scan ExistingRDD[city#870,region#871]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 8"
      ],
      "metadata": {
        "id": "Fno5Lfu6NMwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 — Create the classification logic using UDF\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# UDF to classify order value\n",
        "def classify_amount(amount):\n",
        "    if amount is None:\n",
        "        return None\n",
        "    if amount >= 80000:\n",
        "        return \"High\"\n",
        "    elif amount >= 40000:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "classify_udf = F.udf(classify_amount, StringType())\n"
      ],
      "metadata": {
        "id": "SrUy0gKjLzPf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Add the new column order_value_category\n",
        "df8 = df7_joined.withColumn(\"order_value_category\",\n",
        "                            classify_udf(F.col(\"amount_clean\")))\n",
        "\n",
        "df8.select(\"order_id\", \"amount_clean\", \"order_value_category\").show(15, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFOpC3tmNL5Q",
        "outputId": "bd9869bf-5c83-4e49-ac5c-e6ea61c6bf81"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+--------------------+\n",
            "|order_id   |amount_clean|order_value_category|\n",
            "+-----------+------------+--------------------+\n",
            "|ORD00000001|35430       |Low                 |\n",
            "|ORD00000007|45362       |Medium              |\n",
            "|ORD00000008|10563       |Low                 |\n",
            "|ORD00000010|66576       |Medium              |\n",
            "|ORD00000011|50318       |Medium              |\n",
            "|ORD00000012|84768       |High                |\n",
            "|ORD00000014|79469       |Medium              |\n",
            "|ORD00000015|81018       |High                |\n",
            "|ORD00000017|69582       |Medium              |\n",
            "|ORD00000019|NULL        |NULL                |\n",
            "|ORD00000022|48832       |Medium              |\n",
            "|ORD00000023|12000       |Low                 |\n",
            "|ORD00000024|18082       |Low                 |\n",
            "|ORD00000025|58248       |Medium              |\n",
            "|ORD00000028|70675       |Medium              |\n",
            "+-----------+------------+--------------------+\n",
            "only showing top 15 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 — Analyze distribution of the order value categories\n",
        "\n",
        "df8.groupBy(\"order_value_category\") \\\n",
        "   .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "   .orderBy(F.col(\"count\").desc()) \\\n",
        "   .show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fka9C3yPNL1S",
        "outputId": "c1b2f5af-2fc8-411e-e34c-ef2129982d6f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|order_value_category|count |\n",
            "+--------------------+------+\n",
            "|Low                 |121794|\n",
            "|Medium              |111365|\n",
            "|High                |27936 |\n",
            "|NULL                |23905 |\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 9"
      ],
      "metadata": {
        "id": "bWU5hA5LOME5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Task 1 — Convert the cleaned DataFrame (df8) to RDD\n",
        "rdd = df8.rdd"
      ],
      "metadata": {
        "id": "zTQeLUwkODTP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Compute total revenue using reduce()\n",
        "\n",
        "amount_rdd = rdd.map(lambda row: row.amount_clean).filter(lambda x: x is not None)\n",
        "\n",
        "total_revenue_rdd = amount_rdd.reduce(lambda a, b: a + b)\n",
        "\n",
        "print(\"Total Revenue (RDD):\", total_revenue_rdd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF6XVonNNLyl",
        "outputId": "7c66a002-de46-4860-d275-5c30b78e7367"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue (RDD): 11436490724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 — Compute number of orders per city using map + reduceByKey\n",
        "\n",
        "orders_per_city_rdd = (rdd\n",
        "                       .map(lambda row: (row.city_std, 1))\n",
        "                       .reduceByKey(lambda a, b: a + b))\n",
        "\n",
        "orders_per_city_rdd.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE-MRmX4NLvu",
        "outputId": "9ddc1482-c148-47a9-cfe4-f2f34520bcb8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pune', 40883),\n",
              " ('Mumbai', 40612),\n",
              " ('Hyderabad', 41041),\n",
              " ('Delhi', 40854),\n",
              " ('Bangalore', 40311),\n",
              " ('Kolkata', 40563),\n",
              " ('Chennai', 40736)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for city, count in orders_per_city_rdd.collect():\n",
        "    print(city, \"→\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70lAiZ5NLtA",
        "outputId": "2c0add32-fe87-422e-dd71-164fcb91a173"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pune → 40883\n",
            "Mumbai → 40612\n",
            "Hyderabad → 41041\n",
            "Delhi → 40854\n",
            "Bangalore → 40311\n",
            "Kolkata → 40563\n",
            "Chennai → 40736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 10"
      ],
      "metadata": {
        "id": "WtCj6askPLqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 10 – Task 2\n",
        "df8_cached = df8.cache()\n"
      ],
      "metadata": {
        "id": "duYS-JrcNLps"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 10 – Task 3\n",
        "df8_cached.count()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3U1hzbGNLmt",
        "outputId": "3ac7e63d-6196-4927-c791-51091905a52c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "285000"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 4\n",
        "import time\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# FIRST RUN (without warming cache)\n",
        "start = time.time()\n",
        "df8_cached.groupBy(\"city_std\").agg(F.sum(\"amount_clean\")).count()\n",
        "end = time.time()\n",
        "print(\"1st run time (slower – cold cache):\", end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjmUkJO5NLkE",
        "outputId": "9235e9d5-bace-4466-a345-8ffff2ed54da"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st run time (slower – cold cache): 5.382697820663452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SECOND RUN (faster – using cached data)\n",
        "start = time.time()\n",
        "df8_cached.groupBy(\"city_std\").agg(F.sum(\"amount_clean\")).count()\n",
        "end = time.time()\n",
        "print(\"2nd run time (faster – hot cache):\", end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_UD4_QeNLg3",
        "outputId": "6dad0da8-7a59-47a5-ed06-0a5216038e5f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2nd run time (faster – hot cache): 3.4156618118286133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 10 – Task 5\n",
        "df8_cached.unpersist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMyA3TegNLcB",
        "outputId": "76eb9b55-9263-4798-c8d7-0ea5da1da7ee"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_std: string, category_std: string, product_std: string, status_std: string, amount_clean: int, order_date_clean: date, city: string, region: string, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 11"
      ],
      "metadata": {
        "id": "mezmZxZKQXw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "l = df6_completed.alias(\"l\")\n",
        "r = lookup_df.alias(\"r\")\n",
        "\n",
        "df7_joined = (\n",
        "    l.join(F.broadcast(r), F.col(\"l.city_std\") == F.col(\"r.city\"), \"left\")\n",
        "     .drop(F.col(\"r.city\"))  # drop the lookup's 'city' column to avoid duplicate name\n",
        ")\n",
        "\n",
        "# Carry on to Phase 8 and beyond using df7_joined (no duplicate 'city')\n"
      ],
      "metadata": {
        "id": "Ctc8Mr0mQ3OQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove any column(s) literally named 'city'\n",
        "df8_no_dup = df8.drop(\"city\")\n",
        "\n",
        "# Now write Parquet partitioned by city_std\n",
        "parquet_path = \"/content/out/clean_parquet_partitioned_city\"\n",
        "(df8_no_dup\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .partitionBy(\"city_std\")\n",
        " .parquet(parquet_path))\n",
        "\n",
        "print(\"Parquet written to:\", parquet_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAwLpnRrQ8nr",
        "outputId": "6bd20c8f-97b6-4d2f-f3b9-032153c19432"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet written to: /content/out/clean_parquet_partitioned_city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PHASE 11 – Task 1\n",
        "parquet_path = \"/content/out/clean_parquet_partitioned_city\"\n",
        "\n",
        "(df8_no_dup\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .partitionBy(\"city_std\")\n",
        " .parquet(parquet_path))\n",
        "\n",
        "print(\"Parquet written to:\", parquet_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br8SsB3KP802",
        "outputId": "60a3e4ab-4619-4ba2-9f5e-7e8657d9df65"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet written to: /content/out/clean_parquet_partitioned_city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Write aggregated datasets to ORC\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "orc_path_city    = \"/content/out/agg_orc/revenue_by_city\"\n",
        "orc_path_category= \"/content/out/agg_orc/revenue_by_category\"\n",
        "\n",
        "rev_by_city_orc = (df8.groupBy(\"city_std\")\n",
        "                   .agg(F.sum(\"amount_clean\").alias(\"total_revenue\")))\n",
        "\n",
        "rev_by_category_orc = (df8.groupBy(\"category_std\")\n",
        "                       .agg(F.sum(\"amount_clean\").alias(\"total_revenue\")))\n",
        "\n",
        "(rev_by_city_orc.write.mode(\"overwrite\").orc(orc_path_city))\n",
        "(rev_by_category_orc.write.mode(\"overwrite\").orc(orc_path_category))\n",
        "\n",
        "print(\"ORC written to:\")\n",
        "print(\" -\", orc_path_city)\n",
        "print(\" -\", orc_path_category)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onp_v-Z1P8xi",
        "outputId": "7e927946-a69e-4c8f-a784-98ec103c7ece"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORC written to:\n",
            " - /content/out/agg_orc/revenue_by_city\n",
            " - /content/out/agg_orc/revenue_by_category\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 11 – Task 3\n",
        "parquet_back = spark.read.parquet(parquet_path)\n",
        "orc_city_back = spark.read.orc(orc_path_city)\n",
        "orc_cat_back  = spark.read.orc(orc_path_category)\n",
        "\n",
        "print(\"=== Parquet (clean partitioned) ===\")\n",
        "parquet_back.printSchema()\n",
        "print(\"Row count (Parquet):\", parquet_back.count())\n",
        "\n",
        "print(\"\\n=== ORC (rev by city) ===\")\n",
        "orc_city_back.printSchema()\n",
        "print(\"Row count (ORC city):\", orc_city_back.count())\n",
        "\n",
        "print(\"\\n=== ORC (rev by category) ===\")\n",
        "orc_cat_back.printSchema()\n",
        "print(\"Row count (ORC category):\", orc_cat_back.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN7v6It9P8uy",
        "outputId": "99b5e5ce-cead-4f98-f05b-10b68d4e9d76"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Parquet (clean partitioned) ===\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- category_std: string (nullable = true)\n",
            " |-- product_std: string (nullable = true)\n",
            " |-- status_std: string (nullable = true)\n",
            " |-- amount_clean: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            " |-- city_std: string (nullable = true)\n",
            "\n",
            "Row count (Parquet): 285000\n",
            "\n",
            "=== ORC (rev by city) ===\n",
            "root\n",
            " |-- city_std: string (nullable = true)\n",
            " |-- total_revenue: long (nullable = true)\n",
            "\n",
            "Row count (ORC city): 7\n",
            "\n",
            "=== ORC (rev by category) ===\n",
            "root\n",
            " |-- category_std: string (nullable = true)\n",
            " |-- total_revenue: long (nullable = true)\n",
            "\n",
            "Row count (ORC category): 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 4 Compare size on disk:\n",
        "\n",
        "import os\n",
        "\n",
        "csv_path = \"/content/out/clean_csv\"\n",
        "# Note: writing CSV of the entire dataset can be large; proceed anyway for comparison\n",
        "(df8\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .option(\"header\", True)\n",
        " .csv(csv_path))\n",
        "\n",
        "def dir_size_mb(path):\n",
        "    total = 0\n",
        "    for root, _, files in os.walk(path):\n",
        "        for f in files:\n",
        "            fp = os.path.join(root, f)\n",
        "            total += os.path.getsize(fp)\n",
        "    return round(total / (1024*1024), 2)\n",
        "\n",
        "print(\"Sizes (MB):\")\n",
        "print(\"  CSV     :\", dir_size_mb(csv_path))\n",
        "print(\"  Parquet :\", dir_size_mb(parquet_path))\n",
        "print(\"  ORC city:\", dir_size_mb(orc_path_city))\n",
        "print(\"  ORC cat :\", dir_size_mb(orc_path_category))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCk-I0fiP8rz",
        "outputId": "b07c2d2a-1d52-492b-ba9e-f8c3d2b642af"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes (MB):\n",
            "  CSV     : 37.51\n",
            "  Parquet : 6.46\n",
            "  ORC city: 0.0\n",
            "  ORC cat : 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 5\n",
        "import time\n",
        "\n",
        "# CSV re-read\n",
        "t0 = time.time()\n",
        "csv_back = spark.read.option(\"header\", True).csv(csv_path)\n",
        "csv_cnt = csv_back.count()\n",
        "t1 = time.time()\n",
        "\n",
        "# Parquet re-read\n",
        "t2 = time.time()\n",
        "parquet_back2 = spark.read.parquet(parquet_path)\n",
        "parquet_cnt = parquet_back2.count()\n",
        "t3 = time.time()\n",
        "\n",
        "print(f\"CSV  count={csv_cnt},  read+count time: {t1 - t0:.2f}s\")\n",
        "print(f\"PARQ count={parquet_cnt}, read+count time: {t3 - t2:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuEvnBiBP8o-",
        "outputId": "5c50d0d3-08d2-401c-92ba-8fae914de40b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV  count=285000,  read+count time: 0.44s\n",
            "PARQ count=285000, read+count time: 0.32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 12"
      ],
      "metadata": {
        "id": "vpz_90U5Sw8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(df.amount > 50000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "PRY_J-5-P8mW",
        "outputId": "a21fe5b1-5e0c-41e4-f2f6-34e35877320a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2026-01-15 10:55:33.094\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 1 in cell [64]\", \"line\": \"\", \"fragment\": \"__gt__\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o650.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"__gt__\\\" was called from\\nline 1 in cell [64]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor94.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NumberFormatException",
          "evalue": "[CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [64]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-568713430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line\n",
        "df = df.filter(df.amount > 50000).show()\n",
        "breaks because .show() is an action that returns None.\n",
        "When we assign this to df, the DataFrame variable becomes None, not a DataFrame.\n",
        "Therefore Spark cannot apply further transformations on it.\n",
        "The correct pattern is to apply transformations, assign the DataFrame, and call .show() separately."
      ],
      "metadata": {
        "id": "GZA3lF1xTEen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 13\n"
      ],
      "metadata": {
        "id": "hB29mSX4TILx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PHASE 13 – Task 1  Confirm amount_clean is IntegerType\n",
        "df8.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbTx4GJyP8jh",
        "outputId": "849a72af-5e21-4e11-b8e1-03da7f1bb578"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- city_std: string (nullable = true)\n",
            " |-- category_std: string (nullable = true)\n",
            " |-- product_std: string (nullable = true)\n",
            " |-- status_std: string (nullable = true)\n",
            " |-- amount_clean: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 — Confirm order_date_clean is DateType\n",
        "df8.schema[\"order_date_clean\"].dataType"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPC7qDSBP8g5",
        "outputId": "9c3b390d-b16d-4e76-fd72-a2e0cb497640"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DateType()"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for NULLs in critical fields\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "critical_cols = [\n",
        "    \"order_id\",\n",
        "    \"customer_id\",\n",
        "    \"city_std\",\n",
        "    \"category_std\",\n",
        "    \"product_std\",\n",
        "    \"amount_clean\",\n",
        "    \"order_date_clean\",\n",
        "    \"status_std\"\n",
        "]\n",
        "\n",
        "null_checks = df8.select([\n",
        "    F.sum(F.col(c).isNull().cast(\"int\")).alias(c + \"_nulls\")\n",
        "    for c in critical_cols\n",
        "])\n",
        "\n",
        "null_checks.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnK5qa_gP8d-",
        "outputId": "b80da19a-361d-4591-99e8-9744ac1d32a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------------+--------------+------------------+-----------------+------------------+----------------------+----------------+\n",
            "|order_id_nulls|customer_id_nulls|city_std_nulls|category_std_nulls|product_std_nulls|amount_clean_nulls|order_date_clean_nulls|status_std_nulls|\n",
            "+--------------+-----------------+--------------+------------------+-----------------+------------------+----------------------+----------------+\n",
            "|0             |0                |0             |0                 |0                |23905             |2465                  |0               |\n",
            "+--------------+-----------------+--------------+------------------+-----------------+------------------+----------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 — Check row counts through all phases\n",
        "print(\"Raw Count        :\", raw_count)\n",
        "print(\"After Cleaning   :\", clean_count)\n",
        "print(\"After Dedup      :\", dedup_count)\n",
        "print(\"After Completed  :\", completed_count)\n",
        "print(\"Final DF Count   :\", df8.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fxPIjvGP8a_",
        "outputId": "a25f2b04-7088-48e5-d106-55fbe8e358f9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Count        : 300000\n",
            "After Cleaning   : 300000\n",
            "After Dedup      : 300000\n",
            "After Completed  : 285000\n",
            "Final DF Count   : 285000\n"
          ]
        }
      ]
    }
  ]
}