{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zs3KgsuEmb0_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder\\\n",
        "    .appName(\"Day21_ex1\")\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T\n",
        "sales_data = [\n",
        "    (\"TXN001\",\"Delhi \",\"Laptop\",\"Electronics\",\"45000\",\"2024-01-05\",\"Completed\"),\n",
        "    (\"TXN002\",\"Mumbai\",\"Mobile \",\"electronics\",\"32000\",\"05/01/2024\",\"Completed\"),\n",
        "    (\"TXN003\",\"Bangalore\",\"Tablet\",\" Electronics \",\"30000\",\"2024/01/06\",\"Completed\"),\n",
        "    (\"TXN004\",\"Delhi\",\"Laptop\",\"Electronics\",\"\",\"2024-01-07\",\"Cancelled\"),\n",
        "    (\"TXN005\",\"Chennai\",\"Mobile\",\"Electronics\",\"invalid\",\"2024-01-08\",\"Completed\"),\n",
        "    (\"TXN006\",\"Mumbai\",\"Tablet\",\"Electronics\",None,\"2024-01-08\",\"Completed\"),\n",
        "    (\"TXN007\",\"Delhi\",\"Laptop\",\"electronics\",\"45000\",\"09-01-2024\",\"Completed\"),\n",
        "    (\"TXN008\",\"Bangalore\",\"Mobile\",\"Electronics\",\"28000\",\"2024-01-09\",\"Completed\"),\n",
        "    (\"TXN009\",\"Mumbai\",\"Laptop\",\"Electronics\",\"55000\",\"2024-01-10\",\"Completed\"),\n",
        "    (\"TXN009\",\"Mumbai\",\"Laptop\",\"Electronics\",\"55000\",\"2024-01-10\",\"Completed\")\n",
        "]\n",
        "customer_data = [\n",
        "(\"C001\",\"Delhi\",\"Premium\"),\n",
        "(\"C002\",\"Mumbai\",\"Standard\"),\n",
        "(\"C003\",\"Bangalore\",\"Premium\"),\n",
        "(\"C004\",\"Chennai\",\"Standard\"),\n",
        "(\"C005\",\"Mumbai\",\"Premium\")\n",
        "]\n",
        "city_lookup = [\n",
        "(\"Delhi\",\"Tier-1\"),\n",
        "(\"Mumbai\",\"Tier-1\"),\n",
        "(\"Bangalore\",\"Tier-1\"),\n",
        "(\"Chennai\",\"Tier-2\")\n",
        "]\n",
        "\n",
        "sales_schema = T.StructType([\n",
        "    T.StructField(\"txn_id\",     T.StringType(), True),\n",
        "    T.StructField(\"city\",       T.StringType(), True),\n",
        "    T.StructField(\"product\",    T.StringType(), True),\n",
        "    T.StructField(\"category\",   T.StringType(), True),   # raw as-is (mixed case/spaces)\n",
        "    T.StructField(\"amount\",     T.StringType(), True),   # raw string (may be '', 'invalid', None)\n",
        "    T.StructField(\"order_date\", T.StringType(),True),   # mixed formats\n",
        "    T.StructField(\"status\",     T.StringType(), True),   # truncated/variants\n",
        "])\n",
        "\n",
        "customer_schema = T.StructType([\n",
        "    T.StructField(\"customer_id\", T.StringType(), True),\n",
        "    T.StructField(\"city\",        T.StringType(), True),\n",
        "    T.StructField(\"segment\",     T.StringType(), True),\n",
        "])\n",
        "\n",
        "lookup_schema = T.StructType([\n",
        "    T.StructField(\"city\", T.StringType(), False),\n",
        "    T.StructField(\"tier\", T.StringType(), False),\n",
        "])\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
        "customers_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
        "city_lookup_df = spark.createDataFrame(city_lookup, schema=lookup_schema)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jEThXcx_moVY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,to_date,coalesce\n",
        "#clean_df=sales_df.withColumn(\"amount\",col(\"amount\").cast(T.IntegerType()))\n",
        "\n",
        "clean_df=sales_df.withColumn(\"order_date\",coalesce(to_date(col(\"order_date\"),\"dd/MM/yyyy\"),to_date(col(\"order_date\"),\"yyyy-MM-dd\")\n",
        "  )\n",
        ")\n",
        "#clean_df.show()\n"
      ],
      "metadata": {
        "id": "xGFu1TKsqGQ_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "invalid_df=clean_df.filter(col(\"amount\").isNull() | col(\"order_date\").isNull())\n",
        "\n",
        "#invalid_df.show()"
      ],
      "metadata": {
        "id": "LBR3VU9Rst1S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2"
      ],
      "metadata": {
        "id": "T9g80knJuCBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sales_df = sales_df.withColumn(\"city\", F.upper(F.trim(F.col(\"city\")))) \\\n",
        "                   .withColumn(\"product\", F.upper(F.trim(F.col(\"product\")))) \\\n",
        "                  .withColumn(\"category\", F.upper(F.trim(F.col(\"category\"))))\n",
        "sales_df.show()\n",
        "##5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6TLqy9Rt6R3",
        "outputId": "adcd7cab-d871-4a0f-cd9b-d24f233e42aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "|txn_id|     city|product|   category| amount|order_date|   status|\n",
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "|TXN001|    DELHI| LAPTOP|ELECTRONICS|  45000|2024-01-05|Completed|\n",
            "|TXN002|   MUMBAI| MOBILE|ELECTRONICS|  32000|05/01/2024|Completed|\n",
            "|TXN003|BANGALORE| TABLET|ELECTRONICS|  30000|2024/01/06|Completed|\n",
            "|TXN004|    DELHI| LAPTOP|ELECTRONICS|       |2024-01-07|Cancelled|\n",
            "|TXN005|  CHENNAI| MOBILE|ELECTRONICS|invalid|2024-01-08|Completed|\n",
            "|TXN006|   MUMBAI| TABLET|ELECTRONICS|   NULL|2024-01-08|Completed|\n",
            "|TXN007|    DELHI| LAPTOP|ELECTRONICS|  45000|09-01-2024|Completed|\n",
            "|TXN008|BANGALORE| MOBILE|ELECTRONICS|  28000|2024-01-09|Completed|\n",
            "|TXN009|   MUMBAI| LAPTOP|ELECTRONICS|  55000|2024-01-10|Completed|\n",
            "|TXN009|   MUMBAI| LAPTOP|ELECTRONICS|  55000|2024-01-10|Completed|\n",
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sales_df = sales_df.withColumn(\"category\", F.upper(F.trim(F.col(\"category\"))))\n",
        "sales_df.show()##6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cNoCrGPuJLQ",
        "outputId": "f81b9f51-3ce4-4bfc-dfb7-8e75eb513e55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "|txn_id|     city|product|   category| amount|order_date|   status|\n",
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "|TXN001|    DELHI| LAPTOP|ELECTRONICS|  45000|2024-01-05|Completed|\n",
            "|TXN002|   MUMBAI| MOBILE|ELECTRONICS|  32000|05/01/2024|Completed|\n",
            "|TXN003|BANGALORE| TABLET|ELECTRONICS|  30000|2024/01/06|Completed|\n",
            "|TXN004|    DELHI| LAPTOP|ELECTRONICS|       |2024-01-07|Cancelled|\n",
            "|TXN005|  CHENNAI| MOBILE|ELECTRONICS|invalid|2024-01-08|Completed|\n",
            "|TXN006|   MUMBAI| TABLET|ELECTRONICS|   NULL|2024-01-08|Completed|\n",
            "|TXN007|    DELHI| LAPTOP|ELECTRONICS|  45000|09-01-2024|Completed|\n",
            "|TXN008|BANGALORE| MOBILE|ELECTRONICS|  28000|2024-01-09|Completed|\n",
            "|TXN009|   MUMBAI| LAPTOP|ELECTRONICS|  55000|2024-01-10|Completed|\n",
            "|TXN009|   MUMBAI| LAPTOP|ELECTRONICS|  55000|2024-01-10|Completed|\n",
            "+------+---------+-------+-----------+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "sales_df = sales_df.withColumn(\"amount_int\", F.regexp_extract(F.col(\"amount\"), r\"\\d+\", 0).cast(\"int\")) \\\n",
        "                   .withColumn(\"amount_invalid\", F.col(\"amount_int\").isNull())\n",
        "sales_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "CgpV1HD_uQrF",
        "outputId": "407ffa95-f9a4-4590-f751-940832f01c4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2025-12-23 05:41:29.350\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 3 in cell [19]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o420.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 3 in cell [19]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NumberFormatException",
          "evalue": "[CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 3 in cell [19]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1883181759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msales_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount_int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\d+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount_invalid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount_int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 3 in cell [19]\n"
          ]
        }
      ]
    }
  ]
}